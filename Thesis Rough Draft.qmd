---
title: "Paper Rough Draft"
author: "Daniel Krasnov"
format: revealjs
editor: visual
---

```{r Helper Functions, echo=FALSE}
source("./GBR-NMF code.R")

library(runjags)
library(coda)
library(bayesplot)

summar_A <- function(A, n = 3, func) {
  res <- list()
  for (i in 1:n) {
    a1 <- A[, grep(paste0("A\\[", i, ","), names(A))]
    a1_mean <- apply(a1, MARGIN = 2, func)
    res <- c(res, list(a1_mean))
  }
  
  res
}



summar_S <- function(S, n = 4, func) {
  res <- list()
  for (i in 1:n) {
    s1 <- S[, grep(paste0("S\\[", i, ","), names(S))]
    s1_mean <- apply(s1, MARGIN = 2, func)
    res <- c(res, list(s1_mean))
  }
  res
}

get_AS_hat <- function(A,
                       S,
                       estimate = "median",
                       na = 15,
                       ns = 4) {
  switch (
    estimate,
    median = list(
      A = summar_A(A, na, func = median),
      S = summar_S(S, ns, func = median)
    ),
    mode = list(
      A = summar_A(A, na, func = mode),
      S = summar_S(S, ns, func = mode)
    ),
    mean = list(
      A = summar_A(A, na, func = mean),
      S = summar_S(S, ns, func = mean)
    ),
  )
}

scale_row_S <- function(row) {
  min_val <- min(row)
  max_val <- max(row)
  (row - min_val) / (max_val - min_val)
  # row/sum(row)
}

scale_row_A <- function(row) {
  # min_val <- min(row)
  # max_val <- max(row)
  # (row - min_val) / (max_val - min_val)
  row / sum(row)
}

mode <- function(x) {
  unique_x <- unique(x)
  unique_x[which.max(tabulate(match(x, unique_x)))]
}

plot_s <- function(S, n = 4, main) {
  par(mfrow = c(2, 2))
  for (i in 1:n) {
    plot(S[[i]],
         lty = 1,
         type = "l",
         main = main)
  }
}


summar_A <- function(A, n = 3, func) {
  res <- list()
  for (i in 1:n) {
    a1 <- A[, grep(paste0("A\\[", i, ","), names(A))]
    a1_mean <- apply(a1, MARGIN = 2, func)
    res <- c(res, list(a1_mean))
  }
  
  res
}



summar_S <- function(S, n = 4, func) {
  res <- list()
  for (i in 1:n) {
    s1 <- S[, grep(paste0("S\\[", i, ","), names(S))]
    s1_mean <- apply(s1, MARGIN = 2, func)
    res <- c(res, list(s1_mean))
  }
  res
}

get_AS_hat <- function(A,
                       S,
                       estimate = "median",
                       na = 15,
                       ns = 4) {
  switch (
    estimate,
    median = list(
      A = summar_A(A, na, func = median),
      S = summar_S(S, ns, func = median)
    ),
    mode = list(
      A = summar_A(A, na, func = mode),
      S = summar_S(S, ns, func = mode)
    ),
    mean = list(
      A = summar_A(A, na, func = mean),
      S = summar_S(S, ns, func = mean)
    ),
  )
}
```


# Introduction

Something here about what Raman data is, why we should analyze it, and why a Bayesian approach is warranted.

# Literature Review

Whats been done to analyze Raman data. I want to emphasize that a Bayesian approach will fix whatever is wrong with what's been done.

Whats been done to analyze Raman data using a Bayesian approach.

Talk about GBR-NMF. Talk about why its great but also not great. Talk about how a Bayesian extension will help incorporate uncertainty estimation.

Talk about what my contributions are:

1. Bayesian extension of GBR-NMF
2. Sensitivity analysis of A prior distribution (Dirichlet, Gamma, Exponential)
3. Open source JAGS code, nothing online with implementations of these models.

# Background

## GBR-NMF

Introduction GBR-NMF properly

## BPSS

Introduce BPSS properly

The following comes from [@brie2016bayesian]. Consider the following model:

$$\mathbf{X} = \mathbf{AS} + \mathbf{E}$$

-   $\mathbf{X}_{m \times n}$ is the data matrix where row vectors are the spectra.
-   $\mathbf{A}_{m \times p}$ is the mixing matrix with its column vectors representing the mixing coefficients of each pure component.
-   $\mathbf{S}_{p \times n}$ is the spectra matrix where each row vector is one of the $p$ pure spectra.
-   $\mathbf{E}_{m \times n}$ is the additive noise matrix.
-   It is assumed $\mathbf{A}$ and $\mathbf{S}$ are independent.

Solutions are not unique therefore prior information on the pure component spectra and concentration profiles should be included.

Each noise sequence (row vector of $\mathbf{E}$) is assumed iid Gaussian with zero mean and constant variance within a row. The prior of $\mathbf{E}$ is given as

$$p(\mathbf{E}|\theta_1)=\prod^m_{i=1}\prod^n_{k=1}\mathcal{N}(E_{(i,k)};0,\sigma^2_i)$$

-   $\theta_1 = [\sigma_1^2,\dots,\sigma_m^2]^T$.

The pure component spectra are considered mutually independent and identically distribution. Each pure spectrum (row of $\mathbf{S}$) is assumed Gamma with hyperparameters constant for each spectrum. The prior of $\mathbf{S}$ is given as

$$p(\mathbf{S}|\theta_2)=\prod^n_{j=1}\prod^n_{k=1}\mathcal{G}(S_{(j,k)};\alpha_j,\beta_j)$$

-   $\theta_2 = [\alpha_1,\dots,\alpha_p,\beta_1,\dots,\beta_p]^T$.

Every column of the mixing matrix is assumed Gamma with hyperparameters constant within columns. The prior of $\mathbf{A}$ is given as

$$p(\mathbf{A}|\theta_3)=\prod^m_{i=1}\prod^p_{j=1}\mathcal{G}(A_{(i,j);\gamma_j,\delta_j})$$

-   $\theta_3=[\gamma_1,\dots,\gamma_p,\delta_1,\dots,\delta_p]^T$

The prior for the variance hyperparameter of $\mathbf{E}$ is a $\mathcal{G}(2,\epsilon)$ assigned to $\frac{1}{\sigma^2_i}$ where $\epsilon$ is a small number like $10^{-1}$. The hyperparameters of both $\mathbf{A}$ and $\mathbf{S}$ are $\mathcal{G}(2,\epsilon)$.

The likelihood of the model is given by

$$p(\mathbf{X|\mathbf{S},\mathbf{A},\theta_1})\propto \prod^m_{i=1}\prod^n_{k=1}\left(\frac{1}{\sigma_i}\right)^n\text{exp} \left[ -\frac{1}{2\sigma_i^2}\left(X_{(i,k)}-[\mathbf{AS}]_{(i,k)}\right)^2\right]$$

[@brie2016bayesian] outline several estimators to use for parameters. The joint maximum a posteriori (JMAP) estimates is obtained through:

$$\left(\mathbf{\hat{S}}_{JMAP},\mathbf{\hat{A}}_{JMAP}\right) = \underset{\mathbf{S},\mathbf{A}}{\text{argmax }}p(\mathbf{S},\mathbf{A}|\mathbf{X})$$

An equivalent definition is through the Bayesian interpretation of penalized least squares estimation methods

$$J(\mathbf{S},\mathbf{A})=-\text{log }p(\mathbf{X}|\mathbf{S},\mathbf{A})-\text{log }p(\mathbf{S})-\text{log }p(\mathbf{A})$$ The marginal maximum a posterior (MMAP) estimate are obtained through integrating out either $\mathbf{S}$ or $\mathbf{A}$ and maximizing either posterior marginal distributions $p(\mathbf{S}|\mathbf{X})$ or $p(\mathbf{A}|\mathbf{X})$.

The marginal posterior mean (MPM) estimates are obtained from the mean of the marginal posterior distributions $p(\mathbf{S}|\mathbf{X})$ and $p(\mathbf{A}|\mathbf{X})$.

Talk about how the literature doesn't discuss other distirbutions on A so an exploaration is warranted there.

# Simulations 

## The Case for Informative Initalizations

To investigate the behavior of variations of BPSS the following simulation were investigate. We simulate each signal as a mixture of lorentzian peaks (single raman signals are basically those [citation here]). We then generate a mixture of these mixtures with addiditve proportiosn of $0.6$ and $0.4$. This results in the following simulated signal: 

```{r}
# Load necessary libraries
library(ggplot2)

# Function to generate a single Lorentzian peak
lorentzian_peak <- function(x, x0, intensity, width) {
  (intensity * (width^2)) / ((x - x0)^2 + width^2)
}

# Function to generate a spectrum
generate_spectrum <- function(num_peaks, peak_positions, peak_intensities, peak_widths, x_values) {
  spectrum <- rep(0, length(x_values))
  for (i in 1:num_peaks) {
    spectrum <- spectrum + lorentzian_peak(x_values, peak_positions[i], peak_intensities[i], peak_widths[i])
  }
  # Add noise
  noise <- rnorm(length(spectrum), mean = 0, sd = 0.5)
  spectrum <- spectrum + noise
  return(spectrum)
}

# Parameters
set.seed(87460945) # For reproducibility
x_values <- seq(100, 3000, by = 1) # x-axis (wavenumber)

# Spectrum 1
num_peaks_1 <- 5
peak_positions_1 <- sample(100:3000, num_peaks_1, replace = FALSE)
peak_intensities_1 <- runif(num_peaks_1, min = 10, max = 100)
peak_widths_1 <- runif(num_peaks_1, min = 10, max = 50)
spectrum_1 <- generate_spectrum(num_peaks_1, peak_positions_1, peak_intensities_1, peak_widths_1, x_values)

# Spectrum 2 (different parameters for variety)
num_peaks_2 <- 7
peak_positions_2 <- sample(100:3000, num_peaks_2, replace = FALSE)
peak_intensities_2 <- runif(num_peaks_2, min = 20, max = 120)
peak_widths_2 <- runif(num_peaks_2, min = 20, max = 60)
spectrum_2 <- generate_spectrum(num_peaks_2, peak_positions_2, peak_intensities_2, peak_widths_2, x_values)

# Mix the spectra with specific proportions
proportion_1 <- 0.6
proportion_2 <- 0.4
mixed_spectrum <- (spectrum_1 * proportion_1) + (spectrum_2 * proportion_2)

# Plotting
df1 <- data.frame(Wavenumber = x_values, Intensity = spectrum_1, Spectrum = 'Spectrum 1')
df2 <- data.frame(Wavenumber = x_values, Intensity = spectrum_2, Spectrum = 'Spectrum 2')
df_mixed <- data.frame(Wavenumber = x_values, Intensity = mixed_spectrum, Spectrum = 'Mixed Spectrum')
df <- rbind(df1, df2, df_mixed)

ggplot(df, aes(x = Wavenumber, y = Intensity, color = Spectrum)) +
  geom_line() +
  theme_minimal() +
  labs(title = "Simulated Raman Spectra", x = "Wavenumber", y = "Intensity") +
  scale_color_manual(values = c('Spectrum 1' = 'blue', 'Spectrum 2' = 'green', 'Mixed Spectrum' = 'red'))

```

Spectra 1 adn 2 are the constitute signals that make up the mixed spectrum. First consider the following solutions without initialization on S or A for each model.

### S Gamma unitliazed S and A

The following models are the result of an S matrix with gamma random variables and an A matrix with Gamma random variables. No prior information is incorporated into the model.

```{r}
E <- 1 * 10 ^ -3
p <- 2
m <- 1
n <- length(mixed_spectrum)
mat <- as.matrix(mixed_spectrum) |> t()
mat <- t(apply(mat, 1, scale_row_S)) + E
# 
# data.list = list(
#   X =  mat,
#   m = m,
#   n = n,
#   p = p,
#   E = E,
#   alpha_s = matrix(
#     rep(0.01, n * p),
#     nrow = p,
#     ncol = n,
#     byrow = T
#   ),
#   beta_s = matrix(
#     rep(0.01, n * p),
#     nrow = p,
#     ncol = n,
#     byrow = T
#   ),
#   alpha_a = matrix(rep(0.01, p)) |> t(),
#   beta_a = matrix(rep(0.01, p)) |> t()
# )
# 
# params <- c("A", "S")
# 
# inits1 <-
#   dump.format(list(
#     A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
#     S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
#     .RNG.name = "base::Wichmann-Hill",
#     .RNG.seed = 87460945
#   ))
# inits2 <-
#   dump.format(
#     list(
#       A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
#       S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
#       .RNG.name = "base::Marsaglia-Multicarry",
#       .RNG.seed = 874609
#     )
#   )
# inits3 <-
#   dump.format(list(
#     A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
#     S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
#     .RNG.name = "base::Super-Duper",
#     .RNG.seed = 8746
#   ))
# inits4 <-
#   dump.format(list(
#     A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
#     S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
#     .RNG.name = "base::Mersenne-Twister",
#     .RNG.seed = 874
#   ))
# 
# 
# # Run the model
# posterior <- run.jags(
#   model = JAGS_model_S_gamma_A_gamma,
#   data = data.list,
#   monitor = params,
#   inits = c(inits1, inits2, inits3, inits4),
#   n.chains = 4,
#   adapt = 1000,
#   burnin = 5000,
#   sample = 5000,
#   thin = 10,
#   method = "parallel"
# )
# 
# save(posterior, file = "E:\\Thesis R Data\\JAGS_model_S_gamma_A_gamma_sim_uninit.RData")

load("E:\\Thesis R Data\\JAGS_model_S_gamma_A_gamma_sim_uninit.RData")

# Put samples in dataframe
as.mcmc.list(posterior)[[1]] |> as.data.frame() -> AS

AS <-
  rbind(
    as.mcmc.list(posterior)[[2]] |> as.data.frame(),
    as.mcmc.list(posterior)[[3]] |> as.data.frame(),
    as.mcmc.list(posterior)[[4]] |> as.data.frame()
  )

# head(AS)

# Extract matrices
A <- AS[, (1:p)]
S <- AS[, ((p+1):ncol(AS))]
```

With uninformative prior information set for A and S the model will converge to one of infinitely many solutions to the matrix decomposition.


```{r}
library(gridExtra)
# AS_median <- get_AS_hat(A, S, "median")
# AS_mode <- get_AS_hat(A, S, "mode")
# AS_mean <- get_AS_hat(A, S, "mean")


get_plots_95_ci_spec <- function(p, S, func = "median",n=2) {
  # n <- 3
  res <- list()
  for (i in 1:n) {
    s1 <- S[, grep(paste0("S\\[", i, ","), names(S))]
    s1_mean <- apply(s1, MARGIN = 2, func)
    s1_lower <-
      apply(s1, MARGIN = 2, function(x)
        quantile(x, probs = 0.025))
    s1_upper <-
      apply(s1, MARGIN = 2, function(x)
        quantile(x, probs = 0.975))
    res <-
      c(res,
        list(
          spectra = s1_mean,
          lower = s1_lower,
          upper = s1_upper
        ))
  }
  res
}

plot_95_ci_spec <- function(res, wavelength, spec_num){
  
  
  
i <- switch(as.character(spec_num),
            '1' = 1,
            '2' = 4,
            '3' = 7,
            '4' = 10)
  
   # Common x-axis values
  spectrum <- res[[i]]  # Actual spectral data
  lower_bound <-
    res[[i+1]]  # Lower credible interval (replace with your data)
  upper_bound <-
    res[[i+2]]  # Upper credible interval (replace with your data)
  
  # Combine into a data frame
  data <- data.frame(wavelength, spectrum, lower_bound, upper_bound)
  
  g1 <- ggplot(data, aes(x = wavelength)) +
    geom_ribbon(aes(ymin = lower_bound, ymax = upper_bound),
                fill = "red",
                alpha = 0.5) +
    geom_line(aes(y = spectrum), color = "blue") +  # Plot the spectrum as a line
    theme_minimal() +  # Use a minimal theme
    labs(title = paste0("Median Spectrum ", spec_num," 95% CI "),
         x = "Wavelength",
         y = "Intensity")
  g1

}

res <- get_plots_95_ci_spec(2,S,"median",2)
wavelength <-  1:length(mixed_spectrum)

g1 <- plot_95_ci_spec(res, wavelength, 1)
g2 <- plot_95_ci_spec(res, wavelength, 2)



grid.arrange(g1, g2, nrow = 1, ncol = 2)
# grid.arrange(g3, g4, nrow = 1, ncol = 2)
```

Didn't find different signals.

```{r}
# ggplot(df1, aes(x = Wavenumber, y = Intensity, color = Spectrum)) +
#   geom_line() +
#   theme_minimal() +
#   labs(title = "Simulated Raman Spectra", x = "Wavenumber", y = "Intensity") +
#   scale_color_manual(values = c('Spectrum 1' = 'blue', 'Spectrum 2' = 'green', 'Mixed Spectrum' = 'red'))
# 
# ggplot(df2, aes(x = Wavenumber, y = Intensity, color = Spectrum)) +
#   geom_line() +
#   theme_minimal() +
#   labs(title = "Simulated Raman Spectra", x = "Wavenumber", y = "Intensity") +
#   scale_color_manual(values = c('Spectrum 1' = 'blue', 'Spectrum 2' = 'green', 'Mixed Spectrum' = 'red'))


```


```{r}
# AS_hat <- get_AS_hat(A,
#                      S,
#                      estimate = "median",
#                      na = 1,
#                      ns = 2)

get_A_values_and_median <-  function(A, n = 2,func) {
  res <- list()
  for (i in 1:n) {
    a1 <- A[, grep(paste0("A\\[", i, ","), names(A))]
    a1_mean <- apply(a1, MARGIN = 2, func)
    res <- c(res, list("est" = a1_mean, "values" = a1))
  }
  
  res
}


get_plots_A_hist <- function(A_list,i){
  # res <- list()
  # for (i in 1:length(A_list$est)) {
  # Create the density plot
g1 <- ggplot(A_list$values, aes(x =A_list$values[,i])) +
  geom_density(fill = "blue", alpha = 0.5) + # Density plot with semi-transparent fill
  geom_vline(xintercept = A_list$est[i], color = "red", linetype = "dashed", size = 1) + # Vertical line
  theme_minimal() + # Using a minimal theme for aesthetics
  labs(title = paste("Density Plot of A Values for index", i), 
       x = "Values", 
       y = "Density")
   
   # res <- c(res,g1)
  # }
 g1
}

A_list <- get_A_values_and_median(A, n = 1,"median")
g1 <- get_plots_A_hist(A_list,i=1)
g2 <- get_plots_A_hist(A_list,i=2)

grid.arrange(g1, g2, nrow = 1, ncol = 2)
```

Fairly equivalent solutions.

Given that there are infinetly many solutions it is unsurprising we have poor convergence:

```{r}
plot(posterior,vars=c("S[1,1]"))
```

```{r}
plot(posterior,vars=c("A[1,1]"))
```

Clearly prior information is required to place the model near an optimal solution. 

### GBR-NMF initalization

Consider now using GBR-NMF as an initialization. We place the expectations of each random variable to equal their GBR-NMF initialization and set the variance equal to 1 for each gamma random variable.

```{r}
bases <- matrix(c(df1$Intensity,df2$Intensity), nrow = 2, ncol = length(df2$Intensity), byrow = T)
mat <- as.matrix(mixed_spectrum) |> t()
mat <- t(apply(mat, 1, scale_row_S)) + E
nmf.out <- cnmf(mat, maxit = 10000, q = 2, s = bases)

x <- nmf.out$x
ed <- nmf.out$ed
w <- nmf.out$w
a <- nmf.out$a
s <- nmf.out$s
reconstructedx <- nmf.out$reconstructedx
s_init <- nmf.out$s_init

E <- 1 * 10 ^ -3

expectations_s <-
  t(apply(s, 1, scale_row_S)) + E # added here for numerical reasons

var <- 1

alpha_s <- expectations_s ^ 2 / var
beta_s <- expectations_s / var
beta_s[beta_s == 0] <-  E
alpha_s[alpha_s == 0] <-  E

expectations_a <- t(apply(w, 1, scale_row_A)) + E
alpha_a <- expectations_a ^ 2 / var
beta_a <- expectations_a / var
beta_a[beta_a == 0] <-  E
alpha_a[alpha_a == 0] <-  E


p <- 2
m <- 1
n <- ncol(mat)

data.list = list(
  X = mat,
  m = m,
  n = n,
  p = p,
  E = E,
  alpha_s = alpha_s,
  beta_s = beta_s,
  alpha_a = alpha_a,
  beta_a = beta_a
)


params <- c("A", "S")
# 
# inits1 <-
#   dump.format(list(
#     A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
#     S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
#     .RNG.name = "base::Wichmann-Hill",
#     .RNG.seed = 87460945
#   ))
# inits2 <-
#   dump.format(
#     list(
#       A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
#       S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
#       .RNG.name = "base::Marsaglia-Multicarry",
#       .RNG.seed = 874609
#     )
#   )
# inits3 <-
#   dump.format(list(
#     A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
#     S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
#     .RNG.name = "base::Super-Duper",
#     .RNG.seed = 8746
#   ))
# inits4 <-
#   dump.format(list(
#     A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
#     S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
#     .RNG.name = "base::Mersenne-Twister",
#     .RNG.seed = 874
#   ))
# 
# 
# # Run the model
# posterior <- run.jags(
#   model = JAGS_model_S_gamma_A_gamma,
#   data = data.list,
#   monitor = params,
#   inits = c(inits1, inits2, inits3, inits4),
#   n.chains = 4,
#   adapt = 1000,
#   burnin = 5000,
#   sample = 5000,
#   thin = 10,
#   method = "parallel"
# )
# 
# save(posterior, file = "E:\\Thesis R Data\\JAGS_model_S_gamma_A_gamma_sim_gbrnmf_init.RData")

load("E:\\Thesis R Data\\JAGS_model_S_gamma_A_gamma_sim_gbrnmf_init.RData")

# Put samples in dataframe
as.mcmc.list(posterior)[[1]] |> as.data.frame() -> AS

AS <-
  rbind(
    as.mcmc.list(posterior)[[2]] |> as.data.frame(),
    as.mcmc.list(posterior)[[3]] |> as.data.frame(),
    as.mcmc.list(posterior)[[4]] |> as.data.frame()
  )

# Extract matrices
A <- AS[, (1:p)]
S <- AS[, ((p+1):ncol(AS))]
```

```{r}
library(gridExtra)
# AS_median <- get_AS_hat(A, S, "median")
# AS_mode <- get_AS_hat(A, S, "mode")
# AS_mean <- get_AS_hat(A, S, "mean")


get_plots_95_ci_spec <- function(p, S, func = "median") {
  n <- 3
  res <- list()
  for (i in 1:n) {
    s1 <- S[, grep(paste0("S\\[", i, ","), names(S))]
    s1_mean <- apply(s1, MARGIN = 2, func)
    s1_lower <-
      apply(s1, MARGIN = 2, function(x)
        quantile(x, probs = 0.025))
    s1_upper <-
      apply(s1, MARGIN = 2, function(x)
        quantile(x, probs = 0.975))
    res <-
      c(res,
        list(
          spectra = s1_mean,
          lower = s1_lower,
          upper = s1_upper
        ))
  }
  res
}

plot_95_ci_spec <- function(res, wavelength, spec_num){
  
  
  
i <- switch(as.character(spec_num),
            '1' = 1,
            '2' = 4,
            '3' = 7,
            '4' = 10)
  
   # Common x-axis values
  spectrum <- res[[i]]  # Actual spectral data
  lower_bound <-
    res[[i+1]]  # Lower credible interval (replace with your data)
  upper_bound <-
    res[[i+2]]  # Upper credible interval (replace with your data)
  
  # Combine into a data frame
  data <- data.frame(wavelength, spectrum, lower_bound, upper_bound)
  
  g1 <- ggplot(data, aes(x = wavelength)) +
    geom_ribbon(aes(ymin = lower_bound, ymax = upper_bound),
                fill = "red",
                alpha = 0.5) +
    geom_line(aes(y = spectrum), color = "blue") +  # Plot the spectrum as a line
    theme_minimal() +  # Use a minimal theme
    labs(title = paste0("Median Spectrum ", spec_num," 95% CI "),
         x = "Wavelength",
         y = "Intensity")
  g1

}

res <- get_plots_95_ci_spec(3,S,"median")
wavelength <-  1:length(mixed_spectrum)

g1 <- plot_95_ci_spec(res, wavelength, 1)
g2 <- plot_95_ci_spec(res, wavelength, 2)
# g3 <- plot_95_ci_spec(res, wavelength, 3)



grid.arrange(g1, g2, nrow = 1, ncol = 2)
# grid.arrange(g3, g4, nrow = 1, ncol = 2)
```


discuss how in paper tighter bounds around obvious peaks but where there is a lot of overlap lots of uncertainty.

```{r}
A_list <- get_A_values_and_median(A, n = 1,"median")
g1 <- get_plots_A_hist(A_list,i=1)
g2 <- get_plots_A_hist(A_list,i=2)

grid.arrange(g1, g2, nrow = 1, ncol = 2)
```

Correct solutions for both A and S.


```{r}
plot(posterior,vars=c("S[1,1]"))
```

```{r}
plot(posterior,vars=c("A[1,1]"))

```

Fairly good convergence though in a non simulation setting sampler should be run longer and thinned.



```{r}
library(coda)
rhatResults <- gelman.diag(posterior)
ess <- effectiveSize(posterior)
# any(ess < 4000)
rhatResults$mpsrf # sadly not under 1.1 but pretty close
```


## Sensitivity Analysis of S

Does doing gamma or exp matter? Dosing losing variance specifying power matter?

Vary the variance on Gamma to see if it matter if you constraint it.


### Gamma

The previous example for GBR-NMF showed an initialized Gamma-Gamma BPSS performed well on the simulated data set. Commonly a variance of 1 is specified for the Gamma Distribution of S. It would be interesting to see if the model is robust to different variances on the S matrix keeping the variance of the A matrix 1.

```{r}
# Var 0.1 Gamma simulation
bases <- matrix(c(df1$Intensity,df2$Intensity), nrow = 2, ncol = length(df2$Intensity), byrow = T)
mat <- as.matrix(mixed_spectrum) |> t()
mat <- t(apply(mat, 1, scale_row_S)) + E
nmf.out <- cnmf(mat, maxit = 10000, q = 2, s = bases)

x <- nmf.out$x
ed <- nmf.out$ed
w <- nmf.out$w
a <- nmf.out$a
s <- nmf.out$s
reconstructedx <- nmf.out$reconstructedx
s_init <- nmf.out$s_init

E <- 1 * 10 ^ -3

expectations_s <-
  t(apply(s, 1, scale_row_S)) + E # added here for numerical reasons

var <- 0.1

alpha_s <- expectations_s ^ 2 / var
beta_s <- expectations_s / var
beta_s[beta_s == 0] <-  E
alpha_s[alpha_s == 0] <-  E

var <- 1

expectations_a <- t(apply(w, 1, scale_row_A)) + E
alpha_a <- expectations_a ^ 2 / var
beta_a <- expectations_a / var
beta_a[beta_a == 0] <-  E
alpha_a[alpha_a == 0] <-  E


p <- 2
m <- 1
n <- ncol(mat)

data.list = list(
  X = mat,
  m = m,
  n = n,
  p = p,
  E = E,
  alpha_s = alpha_s,
  beta_s = beta_s,
  alpha_a = alpha_a,
  beta_a = beta_a
)


params <- c("A", "S")

inits1 <-
  dump.format(list(
    A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
    S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
    .RNG.name = "base::Wichmann-Hill",
    .RNG.seed = 87460945
  ))
inits2 <-
  dump.format(
    list(
      A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
      S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
      .RNG.name = "base::Marsaglia-Multicarry",
      .RNG.seed = 874609
    )
  )
inits3 <-
  dump.format(list(
    A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
    S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
    .RNG.name = "base::Super-Duper",
    .RNG.seed = 8746
  ))
inits4 <-
  dump.format(list(
    A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
    S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
    .RNG.name = "base::Mersenne-Twister",
    .RNG.seed = 874
  ))


# Run the model
posterior <- run.jags(
  model = JAGS_model_S_gamma_A_gamma,
  data = data.list,
  monitor = params,
  inits = c(inits1, inits2, inits3, inits4),
  n.chains = 4,
  adapt = 1000,
  burnin = 5000,
  sample = 5000,
  thin = 10,
  method = "parallel"
)

save(posterior, file = "E:\\Thesis R Data\\JAGS_model_S_gamma_A_gamma_var_0_point_1.RData")

load("E:\\Thesis R Data\\JAGS_model_S_gamma_A_gamma_var_0_point_1.RData")

# Put samples in dataframe
as.mcmc.list(posterior)[[1]] |> as.data.frame() -> AS

AS <-
  rbind(
    as.mcmc.list(posterior)[[2]] |> as.data.frame(),
    as.mcmc.list(posterior)[[3]] |> as.data.frame(),
    as.mcmc.list(posterior)[[4]] |> as.data.frame()
  )

# Extract matrices
A_point_1 <- AS[, (1:p)]
S_point_1 <- AS[, ((p+1):ncol(AS))]
```

```{r}
# Var 5 Gamma simulation
bases <- matrix(c(df1$Intensity,df2$Intensity), nrow = 2, ncol = length(df2$Intensity), byrow = T)
mat <- as.matrix(mixed_spectrum) |> t()
mat <- t(apply(mat, 1, scale_row_S)) + E
nmf.out <- cnmf(mat, maxit = 10000, q = 2, s = bases)

x <- nmf.out$x
ed <- nmf.out$ed
w <- nmf.out$w
a <- nmf.out$a
s <- nmf.out$s
reconstructedx <- nmf.out$reconstructedx
s_init <- nmf.out$s_init

E <- 1 * 10 ^ -3

expectations_s <-
  t(apply(s, 1, scale_row_S)) + E # added here for numerical reasons

var <- 5

alpha_s <- expectations_s ^ 2 / var
beta_s <- expectations_s / var
beta_s[beta_s == 0] <-  E
alpha_s[alpha_s == 0] <-  E

var <- 1

expectations_a <- t(apply(w, 1, scale_row_A)) + E
alpha_a <- expectations_a ^ 2 / var
beta_a <- expectations_a / var
beta_a[beta_a == 0] <-  E
alpha_a[alpha_a == 0] <-  E


p <- 2
m <- 1
n <- ncol(mat)

data.list = list(
  X = mat,
  m = m,
  n = n,
  p = p,
  E = E,
  alpha_s = alpha_s,
  beta_s = beta_s,
  alpha_a = alpha_a,
  beta_a = beta_a
)


params <- c("A", "S")

inits1 <-
  dump.format(list(
    A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
    S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
    .RNG.name = "base::Wichmann-Hill",
    .RNG.seed = 87460945
  ))
inits2 <-
  dump.format(
    list(
      A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
      S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
      .RNG.name = "base::Marsaglia-Multicarry",
      .RNG.seed = 874609
    )
  )
inits3 <-
  dump.format(list(
    A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
    S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
    .RNG.name = "base::Super-Duper",
    .RNG.seed = 8746
  ))
inits4 <-
  dump.format(list(
    A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
    S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
    .RNG.name = "base::Mersenne-Twister",
    .RNG.seed = 874
  ))


# Run the model
posterior <- run.jags(
  model = JAGS_model_S_gamma_A_gamma,
  data = data.list,
  monitor = params,
  inits = c(inits1, inits2, inits3, inits4),
  n.chains = 4,
  adapt = 1000,
  burnin = 5000,
  sample = 5000,
  thin = 10,
  method = "parallel"
)

save(posterior, file = "E:\\Thesis R Data\\JAGS_model_S_gamma_A_gamma_var_5.RData")

load("E:\\Thesis R Data\\JAGS_model_S_gamma_A_gamma_var_5.RData")

# Put samples in dataframe
as.mcmc.list(posterior)[[1]] |> as.data.frame() -> AS

AS <-
  rbind(
    as.mcmc.list(posterior)[[2]] |> as.data.frame(),
    as.mcmc.list(posterior)[[3]] |> as.data.frame(),
    as.mcmc.list(posterior)[[4]] |> as.data.frame()
  )

# Extract matrices
A5 <- AS[, (1:p)]
S5 <- AS[, ((p+1):ncol(AS))]
```

```{r}
# Var 10 Gamma simulation
bases <- matrix(c(df1$Intensity,df2$Intensity), nrow = 2, ncol = length(df2$Intensity), byrow = T)
mat <- as.matrix(mixed_spectrum) |> t()
mat <- t(apply(mat, 1, scale_row_S)) + E
nmf.out <- cnmf(mat, maxit = 10000, q = 2, s = bases)

x <- nmf.out$x
ed <- nmf.out$ed
w <- nmf.out$w
a <- nmf.out$a
s <- nmf.out$s
reconstructedx <- nmf.out$reconstructedx
s_init <- nmf.out$s_init

E <- 1 * 10 ^ -3

expectations_s <-
  t(apply(s, 1, scale_row_S)) + E # added here for numerical reasons

var <- 10

alpha_s <- expectations_s ^ 2 / var
beta_s <- expectations_s / var
beta_s[beta_s == 0] <-  E
alpha_s[alpha_s == 0] <-  E

var <- 1

expectations_a <- t(apply(w, 1, scale_row_A)) + E
alpha_a <- expectations_a ^ 2 / var
beta_a <- expectations_a / var
beta_a[beta_a == 0] <-  E
alpha_a[alpha_a == 0] <-  E


p <- 2
m <- 1
n <- ncol(mat)

data.list = list(
  X = mat,
  m = m,
  n = n,
  p = p,
  E = E,
  alpha_s = alpha_s,
  beta_s = beta_s,
  alpha_a = alpha_a,
  beta_a = beta_a
)


params <- c("A", "S")

inits1 <-
  dump.format(list(
    A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
    S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
    .RNG.name = "base::Wichmann-Hill",
    .RNG.seed = 87460945
  ))
inits2 <-
  dump.format(
    list(
      A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
      S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
      .RNG.name = "base::Marsaglia-Multicarry",
      .RNG.seed = 874609
    )
  )
inits3 <-
  dump.format(list(
    A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
    S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
    .RNG.name = "base::Super-Duper",
    .RNG.seed = 8746
  ))
inits4 <-
  dump.format(list(
    A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
    S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
    .RNG.name = "base::Mersenne-Twister",
    .RNG.seed = 874
  ))


# Run the model
posterior <- run.jags(
  model = JAGS_model_S_gamma_A_gamma,
  data = data.list,
  monitor = params,
  inits = c(inits1, inits2, inits3, inits4),
  n.chains = 4,
  adapt = 1000,
  burnin = 5000,
  sample = 5000,
  thin = 10,
  method = "parallel"
)

save(posterior, file = "E:\\Thesis R Data\\JAGS_model_S_gamma_A_gamma_var_10.RData")

load("E:\\Thesis R Data\\JAGS_model_S_gamma_A_gamma_var_10.RData")

# Put samples in dataframe
as.mcmc.list(posterior)[[1]] |> as.data.frame() -> AS

AS <-
  rbind(
    as.mcmc.list(posterior)[[2]] |> as.data.frame(),
    as.mcmc.list(posterior)[[3]] |> as.data.frame(),
    as.mcmc.list(posterior)[[4]] |> as.data.frame()
  )

# Extract matrices
A10 <- AS[, (1:p)]
S10 <- AS[, ((p+1):ncol(AS))]
```


### Exponential

Seems like changing the variance does ___. What if we don't constrain the variance at all and do an expoential distribution

```{r}
bases <- matrix(c(df1$Intensity,df2$Intensity), nrow = 2, ncol = length(df2$Intensity), byrow = T)
mat <- as.matrix(mixed_spectrum) |> t()
mat <- t(apply(mat, 1, scale_row_S)) + E
nmf.out <- cnmf(mat, maxit = 10000, q = 2, s = bases)

x <- nmf.out$x
ed <- nmf.out$ed
w <- nmf.out$w
a <- nmf.out$a
s <- nmf.out$s
reconstructedx <- nmf.out$reconstructedx
s_init <- nmf.out$s_init

E <- 1 * 10 ^ -3

expectations_s <-
  1/t(apply(s, 1, scale_row_S)) + E # added here for numerical reasons


var <- 1

expectations_a <- t(apply(w, 1, scale_row_A)) + E
alpha_a <- expectations_a ^ 2 / var
beta_a <- expectations_a / var
beta_a[beta_a == 0] <-  E
alpha_a[alpha_a == 0] <-  E

expectations_s[which(is.infinite(expectations_s))] <- max(expectations_s[-which(is.infinite(expectations_s))])

p <- 2
m <- 1
n <- ncol(mat)

data.list = list(
  X = mat,
  m = m,
  n = n,
  p = p,
  E = E,
  lambda_s = expectations_s,
  alpha_a = alpha_a,
  beta_a = beta_a
)


params <- c("A", "S")

inits1 <-
  dump.format(list(
    A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
    S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
    .RNG.name = "base::Wichmann-Hill",
    .RNG.seed = 87460945
  ))
inits2 <-
  dump.format(
    list(
      A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
      S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
      .RNG.name = "base::Marsaglia-Multicarry",
      .RNG.seed = 874609
    )
  )
inits3 <-
  dump.format(list(
    A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
    S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
    .RNG.name = "base::Super-Duper",
    .RNG.seed = 8746
  ))
inits4 <-
  dump.format(list(
    A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
    S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
    .RNG.name = "base::Mersenne-Twister",
    .RNG.seed = 874
  ))


# Run the model
posterior <- run.jags(
  model = JAGS_model_S_exp_A_gamma,
  data = data.list,
  monitor = params,
  inits = c(inits1, inits2, inits3, inits4),
  n.chains = 4,
  adapt = 1000,
  burnin = 5000,
  sample = 5000,
  thin = 10,
  method = "parallel"
)

save(posterior, file = "E:\\Thesis R Data\\JAGS_model_S_exp_A_gamma_sim_gbrnmf_init.RData")

load("E:\\Thesis R Data\\JAGS_model_S_exp_A_gamma_sim_gbrnmf_init.RData")

# Put samples in dataframe
as.mcmc.list(posterior)[[1]] |> as.data.frame() -> AS

AS <-
  rbind(
    as.mcmc.list(posterior)[[2]] |> as.data.frame(),
    as.mcmc.list(posterior)[[3]] |> as.data.frame(),
    as.mcmc.list(posterior)[[4]] |> as.data.frame()
  )

# Extract matrices
A <- AS[, (1:p)]
S <- AS[, ((p+1):ncol(AS))]
```



## Sensitivity Analysis of A

There is little discussion of the sensitivity of the model to the prior choice of A. The most common choices are Gamma and Exponential distributions. Generally the Gamma distributed models have their shape such that each variable closely resembles an exponential rnadom variable with constrained variance. Of course opting for the exponential distribution removes the possibility of setting the variance of the matrix entry. While both these distirubtions encode the nonnegativity sought out in these models, they fail to act as a proper mixing proportion, often summing well beyond one. Therefore a natural modification to the model is to place a Dirichlet prior on the distribution of A thus enforcing summation to one. In the following convergence properties are assesed for each prior specification and Bayes factors are calculated to perform a sensitivity analysis.

### Exponential

We begin by considering A values with an exponential distribution

... formulas and theory here...

```{r}
bases <- matrix(c(df1$Intensity,df2$Intensity), nrow = 2, ncol = length(df2$Intensity), byrow = T)
mat <- as.matrix(mixed_spectrum) |> t()
mat <- t(apply(mat, 1, scale_row_S)) + E
nmf.out <- cnmf(mat, maxit = 10000, q = 2, s = bases)

x <- nmf.out$x
ed <- nmf.out$ed
w <- nmf.out$w
a <- nmf.out$a
s <- nmf.out$s
reconstructedx <- nmf.out$reconstructedx
s_init <- nmf.out$s_init

E <- 1 * 10 ^ -3

expectations_s <-
  t(apply(s, 1, scale_row_S)) + E # added here for numerical reasons

var <- 1

alpha_s <- expectations_s ^ 2 / var
beta_s <- expectations_s / var
beta_s[beta_s == 0] <-  E
alpha_s[alpha_s == 0] <-  E

expectations_a <- (1/t(apply(w, 1, scale_row_A))) + E



p <- 2
m <- 1
n <- ncol(mat)

data.list = list(
  X = mat,
  m = m,
  n = n,
  p = p,
  E = E,
  alpha_s = alpha_s,
  beta_s = beta_s,
  lambda_a = expectations_a
)


params <- c("A", "S")

# inits1 <-
#   dump.format(list(
#     A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
#     S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
#     .RNG.name = "base::Wichmann-Hill",
#     .RNG.seed = 87460945
#   ))
# inits2 <-
#   dump.format(
#     list(
#       A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
#       S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
#       .RNG.name = "base::Marsaglia-Multicarry",
#       .RNG.seed = 874609
#     )
#   )
# inits3 <-
#   dump.format(list(
#     A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
#     S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
#     .RNG.name = "base::Super-Duper",
#     .RNG.seed = 8746
#   ))
# inits4 <-
#   dump.format(list(
#     A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
#     S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
#     .RNG.name = "base::Mersenne-Twister",
#     .RNG.seed = 874
#   ))
# 
# 
# # Run the model
# posterior <- run.jags(
#   model = JAGS_model_S_gamma_A_exp,
#   data = data.list,
#   monitor = params,
#   inits = c(inits1, inits2, inits3, inits4),
#   n.chains = 4,
#   adapt = 1000,
#   burnin = 5000,
#   sample = 5000,
#   thin = 10,
#   method = "parallel"
# )
# 
# save(posterior, file = "E:\\Thesis R Data\\JAGS_model_S_gamma_A_exp_sim_gbrnmf_init.RData")

load("E:\\Thesis R Data\\JAGS_model_S_gamma_A_exp_sim_gbrnmf_init.RData")

# Put samples in dataframe
as.mcmc.list(posterior)[[1]] |> as.data.frame() -> AS

AS <-
  rbind(
    as.mcmc.list(posterior)[[2]] |> as.data.frame(),
    as.mcmc.list(posterior)[[3]] |> as.data.frame(),
    as.mcmc.list(posterior)[[4]] |> as.data.frame()
  )

# Extract matrices
A <- AS[, (1:p)]
S <- AS[, ((p+1):ncol(AS))]
```

```{r}
res <- get_plots_95_ci_spec(3,S,"median")
wavelength <-  1:length(mixed_spectrum)

g1 <- plot_95_ci_spec(res, wavelength, 1)
g2 <- plot_95_ci_spec(res, wavelength, 2)
# g3 <- plot_95_ci_spec(res, wavelength, 3)



grid.arrange(g1, g2, nrow = 1, ncol = 2)
```

```{r}
A_list <- get_A_values_and_median(A, n = 1,"median")
g1 <- get_plots_A_hist(A_list,i=1)
g2 <- get_plots_A_hist(A_list,i=2)

grid.arrange(g1, g2, nrow = 1, ncol = 2)
```

TO DO support that selecting constraining variance didn't matter vary the gamma variance as well.

```{r}
plot(posterior,vars=c("S[1,1]"))
```

```{r}
plot(posterior,vars=c("A[1,1]"))
```

### Dirichlet

We begin by considering A values with a Dirichlet distribution
6+
... formulas and theory here...

```{r}
bases <- matrix(c(df1$Intensity,df2$Intensity), nrow = 2, ncol = length(df2$Intensity), byrow = T)
mat <- as.matrix(mixed_spectrum) |> t()
mat <- t(apply(mat, 1, scale_row_S)) + E
nmf.out <- cnmf(mat, maxit = 10000, q = 2, s = bases)

x <- nmf.out$x
ed <- nmf.out$ed
w <- nmf.out$w
a <- nmf.out$a
s <- nmf.out$s
reconstructedx <- nmf.out$reconstructedx
s_init <- nmf.out$s_init

E <- 1 * 10 ^ -3

expectations_s <-
  t(apply(s, 1, scale_row_S)) + E # added here for numerical reasons

var <- 1

alpha_s <- expectations_s ^ 2 / var
beta_s <- expectations_s / var
beta_s[beta_s == 0] <-  E
alpha_s[alpha_s == 0] <-  E

expectations_a <- w/sum(w)



p <- 2
m <- 1
n <- ncol(mat)

data.list = list(
  X = mat,
  m = m,
  n = n,
  p = p,
  E = E,
  alpha_s = alpha_s,
  beta_s = beta_s,
  alpha_a = expectations_a
)


params <- c("A", "S")

# inits1 <-
#   dump.format(list(
#     A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
#     S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
#     .RNG.name = "base::Wichmann-Hill",
#     .RNG.seed = 87460945
#   ))
# inits2 <-
#   dump.format(
#     list(
#       A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
#       S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
#       .RNG.name = "base::Marsaglia-Multicarry",
#       .RNG.seed = 874609
#     )
#   )
# inits3 <-
#   dump.format(list(
#     A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
#     S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
#     .RNG.name = "base::Super-Duper",
#     .RNG.seed = 8746
#   ))
# inits4 <-
#   dump.format(list(
#     A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
#     S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
#     .RNG.name = "base::Mersenne-Twister",
#     .RNG.seed = 874
#   ))
# 
# 
# # Run the model
# posterior <- run.jags(
#   model = JAGS_model_S_gamma_A_dir,
#   data = data.list,
#   monitor = params,
#   inits = c(inits1, inits2, inits3, inits4),
#   n.chains = 4,
#   adapt = 10000,
#   burnin = 10000,
#   sample = 10000,
#   thin = 10,
#   method = "parallel"
# )
# 
# save(posterior, file = "E:\\Thesis R Data\\JAGS_model_S_gamma_A_dir_sim_gbrnmf_init.RData")

load("E:\\Thesis R Data\\JAGS_model_S_gamma_A_dir_sim_gbrnmf_init.RData")

# Put samples in dataframe
as.mcmc.list(posterior)[[1]] |> as.data.frame() -> AS

AS <-
  rbind(
    as.mcmc.list(posterior)[[2]] |> as.data.frame(),
    as.mcmc.list(posterior)[[3]] |> as.data.frame(),
    as.mcmc.list(posterior)[[4]] |> as.data.frame()
  )

# Extract matrices
A <- AS[, (1:p)]
S <- AS[, ((p+1):ncol(AS))]
```


```{r}
res <- get_plots_95_ci_spec(3,S,"median")
wavelength <-  1:length(mixed_spectrum)

g1 <- plot_95_ci_spec(res, wavelength, 1)
g2 <- plot_95_ci_spec(res, wavelength, 2)
# g3 <- plot_95_ci_spec(res, wavelength, 3)



grid.arrange(g1, g2, nrow = 1, ncol = 2)
```

```{r}
A_list <- get_A_values_and_median(A, n = 1,"median")
g1 <- get_plots_A_hist(A_list,i=1)
g2 <- get_plots_A_hist(A_list,i=2)

grid.arrange(g1, g2, nrow = 1, ncol = 2)
```

```{r}
plot(posterior,vars=c("S[1,1]"))
```

```{r}
plot(posterior,vars=c("A[1,1]"))
```

Dirichlet messes up the model big time.

TODO run this again to double check

## Robust Likelihoods

It is well known Normal likelihoods often fail to be robust in the presecense of outliers [Gill's book citation]. A strategy suggested in [Gills textbook] is to play a student-T distribution on the likelihood and compare models with varying degrees of freedom. This allows for the investiation of ... need to read more about this. 

Error tails will be fatter which means they may be more robust and the non error terms could get estimated better.

Could also try skewed error.

# Application

## Data

These are the Raman of the pure chemicals as solids:

```{r}
pure_df <-  read.csv(".\\data\\Input Data for NMF\\Input Data for NMF\\1. Misc q=4 and q=4+ pure bases\\4BasesPure_CA-Glu-Gly-Ser.csv",header = FALSE)

titles <- c("Raman Spectrum for CA", "Raman Spectrum for Glu","Raman Spectrum for Gly","Raman Spectrum for Ser")
par(mfrow=c(2,2))
for (i in 1:4) {
plot(1:length(pure_df[i,]),as.numeric(pure_df[i,]),type="l",ylab="Intensity",xlab="Index",main=titles[i])  
}

```

If you dissolve the same chemicals in a solution you get new spectra:

```{r}
pure_diss_df <-  read.csv(".\\data\\Input Data for NMF\\Input Data for NMF\\2. Misc q=4 and q=4+ solution bases\\4BasesNorm_CA-Glu-Gly-Ser.csv",header = FALSE)
plot_pure_spec_dis <- function(){
  

titles <- c("Raman Spectrum for CA", "Raman Spectrum for Glu","Raman Spectrum for Gly","Raman Spectrum for Ser")
par(mfrow=c(2,2))
for (i in 1:4) {
plot(1:length(pure_diss_df[i,]),as.numeric(pure_diss_df[i,]),type="l",ylab="Intensity",xlab="Index",main=titles[i])  
}
}

plot_pure_spec_dis()
```

The following data was the data put into GBR-NMF:

```{r}
misc_gbrnmf <-  read.csv(".\\data\\Input Data for NMF\\Input Data for NMF\\2. Misc q=4 and q=4+ solution bases\\nmf_data_CA_Glu_Gly_Ser.csv",header = FALSE)
head(misc_gbrnmf[,1:6])
```

There are 3 replicates of 5 mixtures of the same 4 solutions at varying concentrations. This dataframe corresponds to A1-A5 as in the paper. Replicate means they remade the solution and took a Raman spectrum.

```{r}
par(mfrow=c(2,2))
titles <- sapply(1:5, function(i) paste0("A", i, "-", 1:3)) |> as.vector()
for (j in c(1,4,7,10,13)) {
  plot(1:length(misc_gbrnmf[j,]),as.numeric(misc_gbrnmf[j,]),type="l",ylab="Intensity",xlab="Index",col=1,lty=1) 
for (i in (j+1):(j+2)) {
  lines(1:length(misc_gbrnmf[i,]),as.numeric(misc_gbrnmf[i,]),col=i,lty=i)
}
legend("topright", legend=titles[j:(j+2)], col=1:3, lty=1:3)
}
```

This structure is followed for the B's (Lipids) and the C's (TCA). They were looking to see if they could recover the concentrations of the solutions from the mixture by telling NMF which bases signals were present. We want to see if there are Bayesian techniques to deconvolve this mixture into individual signals and see if we can tell the concentration.

We are going to do a PBSS on the Raman spectra using JAGS.


TODO

Talk about the implications of a narrow CI

Talk about the implications of a wide CI

run 3 given spectra and see if it can 4

run 4 given spectra looking for 5 and see what happens

# Conclusion

what you did and so what

# Appendix
## Jags code

```{r}
# Gamma Model code
JAGS_model_S_gamma_A_gamma <- "
model {
    # Likelihood
    for (i in 1:m) {
        for (k in 1:n) {
            X[i, k] ~ dnorm(mu[i, k], tau[i])
            mu[i, k] <- inprod(A[i, ], S[, k])
        }
    }

    # Prior distributions for pure spectra (S) and mixing coefficients (A)
    for (j in 1:p) {
        for (k in 1:n) {
            S[j, k] ~ dgamma(alpha_s[j, k], beta_s[j, k]) T(0.001, 1.001)
        }
    }
    
    for (i in 1:m) {
      for (j in 1:p) {
        A[i, j] ~ dgamma(alpha_a[i,j], beta_a[i,j]) T(0.001,1.001)
      }
    }
    

    # Hyperparameters for the Gamma distributions for each element of S
    for (j in 1:p) {
        for (k in 1:n) {
            alpha_s[j, k] ~ dgamma(2, E)
            beta_s[j, k] ~ dgamma(2, E)
        }
    }
      for (i in 1:m) {
          for (j in 1:p) {
            alpha_a[i,j] ~ dgamma(2, E)
            beta_a[i,j] ~ dgamma(2, E)
          }
        }
    

    # Noise variances
    for (i in 1:m) {
        tau[i] ~ dgamma(2, E)
    }
}
"

JAGS_model_S_exp_A_gamma <- "
model {
    # Likelihood
    for (i in 1:m) {
        for (k in 1:n) {
            X[i, k] ~ dnorm(mu[i, k], tau[i])
            mu[i, k] <- inprod(A[i, ], S[, k])
        }
    }
    

    # Prior distributions for pure spectra (S) and mixing coefficients (A)
    for (j in 1:p) {
        for (k in 1:n) {
            S[j, k] ~ dexp(lambda_s[j,k]) T(0.001, 1.001)
        }
    }
    
    for (i in 1:m) {
      for (j in 1:p) {
        A[i, j] ~ dgamma(alpha_a[i,j], beta_a[i,j]) T(0.001,1.001)
      }
    }
    
  
    # Hyperparameters for the Gamma distributions for each element of S
    for (j in 1:p) {
        for (k in 1:n) {
           lambda_s[j,k] ~ dgamma(2, E) # Assuming a gamma prior for the rate parameters of the Exponential distribution
        }
    }
    
      for (i in 1:m) {
          for (j in 1:p) {
            alpha_a[i,j] ~ dgamma(2, E)
            beta_a[i,j] ~ dgamma(2, E)
          }
        }
    

    # Noise variances
    for (i in 1:m) {
        tau[i] ~ dgamma(2, E)
    }
}
"

JAGS_model_S_gamma_A_dir <- "
model {
    # Likelihood
    for (i in 1:m) {
        for (k in 1:n) {
            X[i, k] ~ dnorm(mu[i, k], tau[i])
            mu[i, k] <- inprod(A[i, ], S[, k])
        }
    }

    # Prior distributions for pure spectra (S)
    for (j in 1:p) {
        for (k in 1:n) {
            S[j, k] ~ dgamma(alpha_s[j, k], beta_s[j, k]) T(0.001, 1.001)
        }
    }
    
    # Corrected Dirichlet prior for mixing coefficients (A)
    for (i in 1:m) {
        A[i, 1:p] ~ ddirich(alpha_a[i, 1:p]) # Ensure alpha_a[i, 1:p] is a vector of concentration parameters for the i-th row of A
    }
    
    # Hyperparameters for the Gamma distributions for each element of S and Dirichlet parameters for A
    for (j in 1:p) {
        for (k in 1:n) {
            alpha_s[j, k] ~ dgamma(2, E)
            beta_s[j, k] ~ dgamma(2, E)
        }
    }
    
     for (i in 1:m) {
      for (j in 1:p) {
        alpha_a[i, j] ~ dgamma(2, E)
      }
    }

    # Noise variances
    for (i in 1:m) {
        tau[i] ~ dgamma(2, E)
    }
}
"


JAGS_model_S_gamma_A_exp <- "
model {
    # Likelihood
    for (i in 1:m) {
        for (k in 1:n) {
            X[i, k] ~ dnorm(mu[i, k], tau[i])
            mu[i, k] <- inprod(A[i, ], S[, k])
        }
    }

    # Prior distributions for pure spectra (S)
    for (j in 1:p) {
        for (k in 1:n) {
            S[j, k] ~ dgamma(alpha_s[j, k], beta_s[j, k]) T(0.001, 1.001)
        }
    }
    
    # Prior distributions for mixing coefficients (A) using Exponential distribution
    for (i in 1:m) {
      for (j in 1:p) {
        A[i, j] ~ dexp(lambda_a[i,j]) T(0.001, 1.001)
      }
    }
    
    # Hyperparameters for the Gamma distributions for each element of S
    # and for the Exponential distributions for each element of A
    for (j in 1:p) {
        for (k in 1:n) {
            alpha_s[j, k] ~ dgamma(2, E)
            beta_s[j, k] ~ dgamma(2, E)
        }
    }
    for (i in 1:m) {
        for (j in 1:p) {
            lambda_a[i,j] ~ dgamma(2, E) # Assuming a gamma prior for the rate parameters of the Exponential distribution
        }
    }

    # Noise variances
    for (i in 1:m) {
        tau[i] ~ dgamma(2, E)
    }
}
"




```

## Convergence Diagnostics

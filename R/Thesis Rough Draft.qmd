---
title: "Paper Rough Draft"
author: "Daniel Krasnov"
format: revealjs
editor: visual
execute:
  cache: true
bibliography: sample.bib
---

```{r Helper Functions, echo=FALSE}
source("./GBR-NMF code.R")

library(runjags)
library(coda)
library(bayesplot)

summar_A <- function(A, n = 3, func) {
  res <- list()
  for (i in 1:n) {
    a1 <- A[, grep(paste0("A\\[", i, ","), names(A))]
    a1_mean <- apply(a1, MARGIN = 2, func)
    res <- c(res, list(a1_mean))
  }
  
  res
}



summar_S <- function(S, n = 4, func) {
  res <- list()
  for (i in 1:n) {
    s1 <- S[, grep(paste0("S\\[", i, ","), names(S))]
    s1_mean <- apply(s1, MARGIN = 2, func)
    res <- c(res, list(s1_mean))
  }
  res
}

get_AS_hat <- function(A,
                       S,
                       estimate = "median",
                       na = 15,
                       ns = 4) {
  switch (
    estimate,
    median = list(
      A = summar_A(A, na, func = median),
      S = summar_S(S, ns, func = median)
    ),
    mode = list(
      A = summar_A(A, na, func = mode),
      S = summar_S(S, ns, func = mode)
    ),
    mean = list(
      A = summar_A(A, na, func = mean),
      S = summar_S(S, ns, func = mean)
    ),
  )
}

scale_row_S <- function(row) {
  min_val <- min(row)
  max_val <- max(row)
  (row - min_val) / (max_val - min_val)
  # row/sum(row)
}

scale_row_A <- function(row) {
  # min_val <- min(row)
  # max_val <- max(row)
  # (row - min_val) / (max_val - min_val)
  row / sum(row)
}

mode <- function(x) {
  unique_x <- unique(x)
  unique_x[which.max(tabulate(match(x, unique_x)))]
}

plot_s <- function(S, n = 4, main) {
  par(mfrow = c(2, 2))
  for (i in 1:n) {
    plot(S[[i]],
         lty = 1,
         type = "l",
         main = main)
  }
}


summar_A <- function(A, n = 3, func) {
  res <- list()
  for (i in 1:n) {
    a1 <- A[, grep(paste0("A\\[", i, ","), names(A))]
    a1_mean <- apply(a1, MARGIN = 2, func)
    res <- c(res, list(a1_mean))
  }
  
  res
}



summar_S <- function(S, n = 4, func) {
  res <- list()
  for (i in 1:n) {
    s1 <- S[, grep(paste0("S\\[", i, ","), names(S))]
    s1_mean <- apply(s1, MARGIN = 2, func)
    res <- c(res, list(s1_mean))
  }
  res
}

get_AS_hat <- function(A,
                       S,
                       estimate = "median",
                       na = 15,
                       ns = 4) {
  switch (
    estimate,
    median = list(
      A = summar_A(A, na, func = median),
      S = summar_S(S, ns, func = median)
    ),
    mode = list(
      A = summar_A(A, na, func = mode),
      S = summar_S(S, ns, func = mode)
    ),
    mean = list(
      A = summar_A(A, na, func = mean),
      S = summar_S(S, ns, func = mean)
    ),
  )
}
```

# Introduction

Something here about what Raman data is, why we should analyze it, and why a Bayesian approach is warranted.

# Literature Review

Whats been done to analyze Raman data. I want to emphasize that a Bayesian approach will fix whatever is wrong with what's been done.

Whats been done to analyze Raman data using a Bayesian approach.

Talk about GBR-NMF. Talk about why its great but also not great. Talk about how a Bayesian extension will help incorporate uncertainty estimation.

Talk about what my contributions are:

1.  Bayesian extension of GBR-NMF
2.  Sensitivity analysis of A prior distribution (Dirichlet, Gamma, Exponential)
3.  Open source JAGS code, nothing online with implementations of these models.

# Background

## Group- and Basis-Restricted Non-Negative Matrix Factorization (GBR-NMF)

Introduced by [@shreeves2023nonnegative], Group- and Basis-Restricted Non-Negative Matrix Factorization (GBR-NMF) extends Non-Negative Matrix Factorization (NMF) by allowing the specification of prior information.

NMF is a dimensionality reduction techique that works through iteratively solving the matrix decomposition

$$X = WH + \epsilon$$ where the rows of $H$ provide the latent factors discovered and the rows of $W$ provide the corresponding score of each factor. Updates to each matrix are calculated via

```{=tex}
\begin{align}
h_{ij} &= h_{ij}\frac{(W^TX)_{ij}}{(W^TWH)_{ij}} & w_{ij} = w_{ij}\frac{(XH^T)_{ij}}{(WHH^T)_{ij}}
\end{align}
```
This formulation often results local optima leading to a sub-optimal solutions. To aid in escaping local optima, [@shreeves2023nonnegative] alters NMF by decomposing $X$ according to

$$X = WAS + \epsilon$$ Where $H$ is decomposed into the auxiliary matrix $A$ and the matrix $S$ containing the latent factors. During estimation, constraints are placed on $S$ such that known factors may be specified in $S$ and unknown factors may be left unspecified. Known factors then have their updates skipped allowing for the algorithm to discover the unknown factors while incorporating previous knowledge. The full algorithm is found in [@shreeves2023nonnegative].

TODO do I need the algoirthm here?

## BPSS

Introduce BPSS properly

The following comes from [@brie2016bayesian]. Consider the following model:

$$\mathbf{X} = \mathbf{AS} + \mathbf{E}$$

-   $\mathbf{X}_{m \times n}$ is the data matrix where row vectors are the spectra.
-   $\mathbf{A}_{m \times p}$ is the mixing matrix with its column vectors representing the mixing coefficients of each pure component.
-   $\mathbf{S}_{p \times n}$ is the spectra matrix where each row vector is one of the $p$ pure spectra.
-   $\mathbf{E}_{m \times n}$ is the additive noise matrix.
-   It is assumed $\mathbf{A}$ and $\mathbf{S}$ are independent.

Solutions are not unique therefore prior information on the pure component spectra and concentration profiles should be included.

Each noise sequence (row vector of $\mathbf{E}$) is assumed iid Gaussian with zero mean and constant variance within a row. The prior of $\mathbf{E}$ is given as

$$p(\mathbf{E}|\theta_1)=\prod^m_{i=1}\prod^n_{k=1}\mathcal{N}(E_{(i,k)};0,\sigma^2_i)$$

-   $\theta_1 = [\sigma_1^2,\dots,\sigma_m^2]^T$.

The pure component spectra are considered mutually independent and identically distribution. Each pure spectrum (row of $\mathbf{S}$) is assumed Gamma with hyperparameters constant for each spectrum. The prior of $\mathbf{S}$ is given as

$$p(\mathbf{S}|\theta_2)=\prod^n_{j=1}\prod^n_{k=1}\mathcal{G}(S_{(j,k)};\alpha_j,\beta_j)$$

-   $\theta_2 = [\alpha_1,\dots,\alpha_p,\beta_1,\dots,\beta_p]^T$.

Every column of the mixing matrix is assumed Gamma with hyperparameters constant within columns. The prior of $\mathbf{A}$ is given as

$$p(\mathbf{A}|\theta_3)=\prod^m_{i=1}\prod^p_{j=1}\mathcal{G}(A_{(i,j);\gamma_j,\delta_j})$$

-   $\theta_3=[\gamma_1,\dots,\gamma_p,\delta_1,\dots,\delta_p]^T$

The prior for the variance hyperparameter of $\mathbf{E}$ is a $\mathcal{G}(2,\epsilon)$ assigned to $\frac{1}{\sigma^2_i}$ where $\epsilon$ is a small number like $10^{-1}$. The hyperparameters of both $\mathbf{A}$ and $\mathbf{S}$ are $\mathcal{G}(2,\epsilon)$.

The likelihood of the model is given by

$$p(\mathbf{X|\mathbf{S},\mathbf{A},\theta_1})\propto \prod^m_{i=1}\prod^n_{k=1}\left(\frac{1}{\sigma_i}\right)^n\text{exp} \left[ -\frac{1}{2\sigma_i^2}\left(X_{(i,k)}-[\mathbf{AS}]_{(i,k)}\right)^2\right]$$

[@brie2016bayesian] outline several estimators to use for parameters. The joint maximum a posteriori (JMAP) estimates is obtained through:

$$\left(\mathbf{\hat{S}}_{JMAP},\mathbf{\hat{A}}_{JMAP}\right) = \underset{\mathbf{S},\mathbf{A}}{\text{argmax }}p(\mathbf{S},\mathbf{A}|\mathbf{X})$$

An equivalent definition is through the Bayesian interpretation of penalized least squares estimation methods

$$J(\mathbf{S},\mathbf{A})=-\text{log }p(\mathbf{X}|\mathbf{S},\mathbf{A})-\text{log }p(\mathbf{S})-\text{log }p(\mathbf{A})$$ The marginal maximum a posterior (MMAP) estimate are obtained through integrating out either $\mathbf{S}$ or $\mathbf{A}$ and maximizing either posterior marginal distributions $p(\mathbf{S}|\mathbf{X})$ or $p(\mathbf{A}|\mathbf{X})$.

The marginal posterior mean (MPM) estimates are obtained from the mean of the marginal posterior distributions $p(\mathbf{S}|\mathbf{X})$ and $p(\mathbf{A}|\mathbf{X})$.

Talk about how the literature doesn't discuss other distirbutions on A so an exploaration is warranted there.

# Simulations

Given that BPSS allows for infinitely many solutions to the matrix decomposition, a simulation study was underwent to understand the sensitivity of BPSS to prior and likelihood selection as well as model initialization.

The primary application of Bayesian GBR-NMF is the decomposition of Raman spectra into constitute signals. The simulated dataset was generated to reflect. It is well known that Raman spectra can be represented as Voigt functions, the convolution of a radial basis function and a Lorentzian peak [@moores2016bayesian]. As a simplification the simulated Raman spectra are generated solely as mixtures of Lorezentzians [@moores2016bayesian]:

$$f_L(\nu_j | \ell_p , \gamma_p) = \frac{\gamma_p^2}{(\nu_j - \ell_p)^2 + \gamma^2_p}$$ where $\gamma_p$ is the half-width at half-maximum and $\ell_p$ is the peak frequency. Each peak is generated according to this function giving rise to a single Raman spectrum. Normal noise centered at $0$ with a standard deviation of $0.5$ is added to each point of the signal. Next, a mixture of these signals is generated in the proportions of $0.6$ and $0.4$. This results in the following simulated signal:

```{r}
#| fig-cap: 2 simulated Raman spectra and their mixture are plotted overlapping.

# Load necessary libraries
library(ggplot2)

# Function to generate a single Lorentzian peak
lorentzian_peak <- function(x, x0, intensity, width) {
  (intensity * (width^2)) / ((x - x0)^2 + width^2)
}

# Function to generate a spectrum
generate_spectrum <- function(num_peaks, peak_positions, peak_intensities, peak_widths, x_values) {
  spectrum <- rep(0, length(x_values))
  for (i in 1:num_peaks) {
    spectrum <- spectrum + lorentzian_peak(x_values, peak_positions[i], peak_intensities[i], peak_widths[i])
  }
  # Add noise
  noise <- rnorm(length(spectrum), mean = 0, sd = 0.5)
  spectrum <- spectrum + noise
  return(spectrum)
}

# Parameters
set.seed(87460945) # For reproducibility
x_values <- seq(100, 3000, by = 1) # x-axis (wavenumber)

# Spectrum 1
num_peaks_1 <- 5
peak_positions_1 <- sample(100:3000, num_peaks_1, replace = FALSE)
peak_intensities_1 <- runif(num_peaks_1, min = 10, max = 100)
peak_widths_1 <- runif(num_peaks_1, min = 10, max = 50)
spectrum_1 <- generate_spectrum(num_peaks_1, peak_positions_1, peak_intensities_1, peak_widths_1, x_values)

# Spectrum 2 (different parameters for variety)
num_peaks_2 <- 7
peak_positions_2 <- sample(100:3000, num_peaks_2, replace = FALSE)
peak_intensities_2 <- runif(num_peaks_2, min = 20, max = 120)
peak_widths_2 <- runif(num_peaks_2, min = 20, max = 60)
spectrum_2 <- generate_spectrum(num_peaks_2, peak_positions_2, peak_intensities_2, peak_widths_2, x_values)

# Mix the spectra with specific proportions
proportion_1 <- 0.6
proportion_2 <- 0.4
mixed_spectrum <- (spectrum_1 * proportion_1) + (spectrum_2 * proportion_2)

# Normalize spectra to limit maximum intensity to 1
max_intensity <- max(c(spectrum_1, spectrum_2, mixed_spectrum))
spectrum_1 <- spectrum_1 / max_intensity
spectrum_2 <- spectrum_2 / max_intensity
mixed_spectrum <- mixed_spectrum / max_intensity

# Plotting
df1 <- data.frame(Wavenumber = x_values, Intensity = spectrum_1, Spectrum = 'Spectrum 1')
df2 <- data.frame(Wavenumber = x_values, Intensity = spectrum_2, Spectrum = 'Spectrum 2')
df_mixed <- data.frame(Wavenumber = x_values, Intensity = mixed_spectrum, Spectrum = 'Mixed Spectrum')
df <- rbind(df1, df2, df_mixed)

ggplot(df, aes(x = Wavenumber, y = Intensity, color = Spectrum)) +
  geom_line() +
  theme_minimal() +
  labs(title = "Simulated Raman Spectra", x = "Wavenumber", y = "Intensity") +
  scale_color_manual(values = c('Spectrum 1' = 'blue', 'Spectrum 2' = 'green', 'Mixed Spectrum' = 'red'))

```

```{r}
#| fig-cap: 2 constitue Raman spectra (left) and their combination in the proportions of 0.4 for Spectrum 1 and 0.6 for Spectrum 2

# Load necessary libraries
library(ggplot2)
library(gridExtra)

# Combine df1 and df2 for the overlapping plot
df_overlap <- rbind(df1, df2)

# Overlapping Spectrum 1 and Spectrum 2
plot_overlap <- ggplot(df_overlap, aes(x = Wavenumber, y = Intensity, color = Spectrum)) +
  geom_line() +
  theme_minimal() +
  labs(title = "Spectrum 1 and 2 Overlapped", x = "Wavenumber", y = "Intensity") +
  scale_color_manual(values = c('Spectrum 1' = 'blue', 'Spectrum 2' = 'green'))

# Mixed Spectrum plot
plot_mixed_spectrum <- ggplot(df_mixed, aes(x = Wavenumber, y = Intensity)) +
  geom_line(color = 'red') +
  theme_minimal() +
  labs(title = "Mixed Spectrum", x = "Wavenumber", y = "Intensity")

# Arrange the plots in a grid
grid.arrange(plot_overlap, plot_mixed_spectrum, ncol = 2)

```

## The Case for Informative Initalizations

Solutions to the matrix decomposition (formula number) are not unique [@brie2016bayesian]. Therefore it will be critical to provide good initialization of both the mixing matrix $A$ and signal matrix $S$. In the following section, this claim is justified.

### S Gamma unitliazed S and A

Consider a BPSS model with gamma distributed mixing and spectra matrices. With no prior information, the following decomposition is learned:

```{r}
# E <- 1 * 10 ^ -3
# p <- 2
# m <- 1
# n <- length(mixed_spectrum)
# mat <- as.matrix(mixed_spectrum) |> t()
# mat <- t(apply(mat, 1, scale_row_S)) + E
# 
# data.list = list(
#   X =  mat,
#   m = m,
#   n = n,
#   p = p,
#   E = E,
#   alpha_s = matrix(
#     rep(0.01, n * p),
#     nrow = p,
#     ncol = n,
#     byrow = T
#   ),
#   beta_s = matrix(
#     rep(0.01, n * p),
#     nrow = p,
#     ncol = n,
#     byrow = T
#   ),
#   alpha_a = matrix(rep(0.01, p)) |> t(),
#   beta_a = matrix(rep(0.01, p)) |> t()
# )
# 
# params <- c("A", "S")
# 
# inits1 <-
#   dump.format(list(
#     A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
#     S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
#     .RNG.name = "base::Wichmann-Hill",
#     .RNG.seed = 87460945
#   ))
# inits2 <-
#   dump.format(
#     list(
#       A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
#       S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
#       .RNG.name = "base::Marsaglia-Multicarry",
#       .RNG.seed = 874609
#     )
#   )
# inits3 <-
#   dump.format(list(
#     A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
#     S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
#     .RNG.name = "base::Super-Duper",
#     .RNG.seed = 8746
#   ))
# inits4 <-
#   dump.format(list(
#     A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
#     S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
#     .RNG.name = "base::Mersenne-Twister",
#     .RNG.seed = 874
#   ))
# 
# 
# # Run the model
# posterior <- run.jags(
#   model = JAGS_model_S_gamma_A_gamma,
#   data = data.list,
#   monitor = params,
#   inits = c(inits1, inits2, inits3, inits4),
#   n.chains = 4,
#   adapt = 1000,
#   burnin = 5000,
#   sample = 5000,
#   thin = 10,
#   method = "parallel"
# )
# 
# save(posterior, file = "E:\\Thesis R Data\\JAGS_model_S_gamma_A_gamma_sim_uninit.RData")

load("E:\\Thesis R Data\\JAGS_model_S_gamma_A_gamma_sim_uninit.RData")

# Put samples in dataframe
as.mcmc.list(posterior)[[1]] |> as.data.frame() -> AS

AS <-
  rbind(
    as.mcmc.list(posterior)[[2]] |> as.data.frame(),
    as.mcmc.list(posterior)[[3]] |> as.data.frame(),
    as.mcmc.list(posterior)[[4]] |> as.data.frame()
  )

# head(AS)

# Extract matrices
A <- AS[, (1:p)]
S <- AS[, ((p+1):ncol(AS))]
```

```{r}
#| fig-cap: 95% Credible interval of 2 learned constitue spectra. Median spectra are plotted in blue.


library(gridExtra)
# AS_median <- get_AS_hat(A, S, "median")
# AS_mode <- get_AS_hat(A, S, "mode")
# AS_mean <- get_AS_hat(A, S, "mean")


get_plots_95_ci_spec <- function(p, S, func = "median",n=2) {
  # n <- 3
  res <- list()
  for (i in 1:n) {
    s1 <- S[, grep(paste0("S\\[", i, ","), names(S))]
    s1_mean <- apply(s1, MARGIN = 2, func)
    s1_lower <-
      apply(s1, MARGIN = 2, function(x)
        quantile(x, probs = 0.025))
    s1_upper <-
      apply(s1, MARGIN = 2, function(x)
        quantile(x, probs = 0.975))
    res <-
      c(res,
        list(
          spectra = s1_mean,
          lower = s1_lower,
          upper = s1_upper
        ))
  }
  res
}

plot_95_ci_spec <- function(res, wavelength, spec_num){
  
  
  
i <- switch(as.character(spec_num),
            '1' = 1,
            '2' = 4,
            '3' = 7,
            '4' = 10)
  
   # Common x-axis values
  spectrum <- res[[i]]  # Actual spectral data
  lower_bound <-
    res[[i+1]]  # Lower credible interval (replace with your data)
  upper_bound <-
    res[[i+2]]  # Upper credible interval (replace with your data)
  
  # Combine into a data frame
  data <- data.frame(wavelength, spectrum, lower_bound, upper_bound)
  
  g1 <- ggplot(data, aes(x = wavelength)) +
    geom_ribbon(aes(ymin = lower_bound, ymax = upper_bound),
                fill = "red",
                alpha = 0.5) +
    geom_line(aes(y = spectrum), color = "blue") +  # Plot the spectrum as a line
    theme_minimal() +  # Use a minimal theme
    labs(title = paste0("Median Spectrum ", spec_num," 95% CI "),
         x = "Wavelength",
         y = "Intensity")
  g1

}

res <- get_plots_95_ci_spec(2,S,"median",2)
wavelength <-  1:length(mixed_spectrum)

g1 <- plot_95_ci_spec(res, wavelength, 1)
g2 <- plot_95_ci_spec(res, wavelength, 2)



grid.arrange(g1, g2, nrow = 1, ncol = 2)
```

Plotted are the 95% credible intervals for the model's learned constitute spectra. In blue the median spectra are plotted. Clearly, the model has converged to a poor solution as identical spectra are learned and error bands are large.

```{r}
# AS_hat <- get_AS_hat(A,
#                      S,
#                      estimate = "median",
#                      na = 1,
#                      ns = 2)

get_A_values_and_median <-  function(A, n = 2,func) {
  res <- list()
  for (i in 1:n) {
    a1 <- A[, grep(paste0("A\\[", i, ","), names(A))]
    a1_mean <- apply(a1, MARGIN = 2, func)
    res <- c(res, list("est" = a1_mean, "values" = a1))
  }
  
  res
}


# get_plots_A_hist <- function(A_list,i){
#   # res <- list()
#   # for (i in 1:length(A_list$est)) {
#   # Create the density plot
# g1 <- ggplot(A_list$values, aes(x =A_list$values[,i])) +
#   geom_density(fill = "blue", alpha = 0.5) + # Density plot with semi-transparent fill
#   geom_vline(xintercept = A_list$est[i], color = "red", linetype = "dashed", size = 1) + # Vertical line
#   theme_minimal() + # Using a minimal theme for aesthetics
#   labs(title = paste("Density Plot of A Values for index", i), 
#        x = "Values", 
#        y = "Density")
#    
#    # res <- c(res,g1)
#   # }
#  g1
# }

get_plots_A_hist <- function(A_list, i) {
  # Calculate the density
  dens <- density(A_list$values[,i])
  
  # Calculate the 95% credible interval
  ci <- quantile(A_list$values[,i], probs = c(0.025, 0.975))
  
  # Create a data frame for plotting
  dens_df <- data.frame(x = dens$x, y = dens$y)
  
  # Find y values for the CI bounds to highlight the area under the curve
  ci_df <- dens_df[dens_df$x >= ci[1] & dens_df$x <= ci[2],]
  
  # Create the plot
  g1 <- ggplot() +
    geom_line(data = dens_df, aes(x = x, y = y), color = "blue") + # Full density curve
    geom_area(data = ci_df, aes(x = x, y = y), fill = "blue", alpha = 0.5) + # Highlighted 95% CI area
    geom_vline(xintercept = A_list$est[i], color = "red", linetype = "dashed", size = 1) + # Vertical line for the estimate
    theme_minimal() + # Using a minimal theme for aesthetics
    labs(title = paste("Density Plot of A Values for index", i), 
         x = "Values", 
         y = "Density")
  
  return(g1)
}


A_list <- get_A_values_and_median(A, n = 1,"median")
g1 <- get_plots_A_hist(A_list,i=1)
g2 <- get_plots_A_hist(A_list,i=2)

grid.arrange(g1, g2, nrow = 1, ncol = 2)
```

Bimodal densities are learned for the mixing matrix A. Plotted are empirical densities of MCMC values with the median value plotted as the dotted red line. Clearly this solution is incorrect as the mixing proportions we are searching for are $0.4$ and $0.6$ and after normalization the learned values are $0.49$ and $0.51$ respectively.

Bimodal densities are a direct result of poor convergence. Plotting diagnostics for $A_{1,1}$ reveals that in fact each markov chain has reached a separate stationary distribution with exetreme auto-correlation.

```{r}
plot(posterior,vars=c("A[1,1]"))
```

This is to be expected as since the parameter space is placed randomly via random number generation, the model is likely to become trapped and fail to converge. Thus, prior information is required to place the model within range of an optimal solution.

### GBR-NMF initalization

Consider now using GBR-NMF as an initialization. We place the expectations of each Gamma random variable to equal their GBR-NMF initialization and set the variances equal to $1$.

```{r}
# bases <- matrix(c(df1$Intensity,df2$Intensity), nrow = 2, ncol = length(df2$Intensity), byrow = T)
# mat <- as.matrix(mixed_spectrum) |> t()
# mat <- t(apply(mat, 1, scale_row_S)) + E
# nmf.out <- cnmf(mat, maxit = 10000, q = 2, s = bases)
# 
# x <- nmf.out$x
# ed <- nmf.out$ed
# w <- nmf.out$w
# a <- nmf.out$a
# s <- nmf.out$s
# reconstructedx <- nmf.out$reconstructedx
# s_init <- nmf.out$s_init
# 
# E <- 1 * 10 ^ -3
# 
# expectations_s <-
#   t(apply(s, 1, scale_row_S)) + E # added here for numerical reasons
# 
# var <- 1
# 
# alpha_s <- expectations_s ^ 2 / var
# beta_s <- expectations_s / var
# beta_s[beta_s == 0] <-  E
# alpha_s[alpha_s == 0] <-  E
# 
# expectations_a <- t(apply(w, 1, scale_row_A)) + E
# alpha_a <- expectations_a ^ 2 / var
# beta_a <- expectations_a / var
# beta_a[beta_a == 0] <-  E
# alpha_a[alpha_a == 0] <-  E
# 
# 
# p <- 2
# m <- 1
# n <- ncol(mat)
# 
# data.list = list(
#   X = mat,
#   m = m,
#   n = n,
#   p = p,
#   E = E,
#   alpha_s = alpha_s,
#   beta_s = beta_s,
#   alpha_a = alpha_a,
#   beta_a = beta_a
# )
# 
# 
# params <- c("A", "S")
# 
# inits1 <-
#   dump.format(list(
#     A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
#     S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
#     .RNG.name = "base::Wichmann-Hill",
#     .RNG.seed = 87460945
#   ))
# inits2 <-
#   dump.format(
#     list(
#       A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
#       S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
#       .RNG.name = "base::Marsaglia-Multicarry",
#       .RNG.seed = 874609
#     )
#   )
# inits3 <-
#   dump.format(list(
#     A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
#     S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
#     .RNG.name = "base::Super-Duper",
#     .RNG.seed = 8746
#   ))
# inits4 <-
#   dump.format(list(
#     A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
#     S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
#     .RNG.name = "base::Mersenne-Twister",
#     .RNG.seed = 874
#   ))
# 
# 
# # Run the model
# posterior <- run.jags(
#   model = JAGS_model_S_gamma_A_gamma,
#   data = data.list,
#   monitor = params,
#   inits = c(inits1, inits2, inits3, inits4),
#   n.chains = 4,
#   adapt = 1000,
#   burnin = 5500,
#   sample = 5500,
#   thin = 50,
#   method = "parallel"
# )
# 
# save(posterior, file = "E:\\Thesis R Data\\JAGS_model_S_gamma_A_gamma_sim_gbrnmf_init.RData")

load("E:\\Thesis R Data\\JAGS_model_S_gamma_A_gamma_sim_gbrnmf_init.RData")

# Put samples in dataframe
as.mcmc.list(posterior)[[1]] |> as.data.frame() -> AS

AS <-
  rbind(
    as.mcmc.list(posterior)[[2]] |> as.data.frame(),
    as.mcmc.list(posterior)[[3]] |> as.data.frame(),
    as.mcmc.list(posterior)[[4]] |> as.data.frame()
  )

# Extract matrices
A <- AS[, (1:p)]
S <- AS[, ((p+1):ncol(AS))]
```

```{r}
res <- get_plots_95_ci_spec(2,S,"median",2)
wavelength <-  1:length(mixed_spectrum)

g1 <- plot_95_ci_spec(res, wavelength, 1)
g2 <- plot_95_ci_spec(res, wavelength, 2)



grid.arrange(g1, g2, nrow = 1, ncol = 2)
```

While not perfect in the error bands, median spectra closely resemble their constituent counterparts.

TODO talk to Irene about about how the 95% CI on the left is actually showing the other spectrum in the furthest percentile.

```{r}
A_list <- get_A_values_and_median(A, n = 1,"median")
g1 <- get_plots_A_hist(A_list,i=1)
g2 <- get_plots_A_hist(A_list,i=2)

grid.arrange(g1, g2, nrow = 1, ncol = 2)
```

The normalized median mixing proportions learned in A are `A_list$est[1] / sum(A_list$est)` and `A_list$est[2] / sum(A_list$est)`. Importantly the true values for the mixing proportions are contained within the 95% credible intervals.

```{r}
plot(posterior,vars=c("S[1,1]"))
```

```{r}
plot(posterior,vars=c("A[1,1]"))

```

Fairly good convergence though in a non simulation setting sampler should be run longer and thinned.

```{r}
library(coda)
rhatResults <- gelman.diag(posterior)
ess <- effectiveSize(posterior)
# any(ess < 4000)
rhatResults$mpsrf # sadly not under 1.1 but pretty close
```

## Sensitivity Analysis of S

Does doing gamma or exp matter? Dosing losing variance specifying power matter?

Vary the variance on Gamma to see if it matter if you constraint it.

### Gamma

The previous example for GBR-NMF showed an initialized Gamma-Gamma BPSS performed well on the simulated data set. Commonly a variance of 1 is specified for the Gamma Distribution of S. It would be interesting to see if the model is robust to different variances on the S matrix keeping the variance of the A matrix 1.

```{r}
# Var 0.1 Gamma simulation
# bases <- matrix(c(df1$Intensity,df2$Intensity), nrow = 2, ncol = length(df2$Intensity), byrow = T)
# mat <- as.matrix(mixed_spectrum) |> t()
# mat <- t(apply(mat, 1, scale_row_S)) + E
# nmf.out <- cnmf(mat, maxit = 10000, q = 2, s = bases)
# 
# x <- nmf.out$x
# ed <- nmf.out$ed
# w <- nmf.out$w
# a <- nmf.out$a
# s <- nmf.out$s
# reconstructedx <- nmf.out$reconstructedx
# s_init <- nmf.out$s_init
# 
# E <- 1 * 10 ^ -3
# 
# expectations_s <-
#   t(apply(s, 1, scale_row_S)) + E # added here for numerical reasons
# 
# var <- 0.1
# 
# alpha_s <- expectations_s ^ 2 / var
# beta_s <- expectations_s / var
# beta_s[beta_s == 0] <-  E
# alpha_s[alpha_s == 0] <-  E
# 
# var <- 1
# 
# expectations_a <- t(apply(w, 1, scale_row_A)) + E
# alpha_a <- expectations_a ^ 2 / var
# beta_a <- expectations_a / var
# beta_a[beta_a == 0] <-  E
# alpha_a[alpha_a == 0] <-  E
# 
# 
# p <- 2
# m <- 1
# n <- ncol(mat)
# 
# data.list = list(
#   X = mat,
#   m = m,
#   n = n,
#   p = p,
#   E = E,
#   alpha_s = alpha_s,
#   beta_s = beta_s,
#   alpha_a = alpha_a,
#   beta_a = beta_a
# )
# 
# 
# params <- c("A", "S")
# 
# inits1 <-
#   dump.format(list(
#     A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
#     S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
#     .RNG.name = "base::Wichmann-Hill",
#     .RNG.seed = 87460945
#   ))
# inits2 <-
#   dump.format(
#     list(
#       A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
#       S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
#       .RNG.name = "base::Marsaglia-Multicarry",
#       .RNG.seed = 874609
#     )
#   )
# inits3 <-
#   dump.format(list(
#     A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
#     S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
#     .RNG.name = "base::Super-Duper",
#     .RNG.seed = 8746
#   ))
# inits4 <-
#   dump.format(list(
#     A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
#     S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
#     .RNG.name = "base::Mersenne-Twister",
#     .RNG.seed = 874
#   ))
# 
# 
# # Run the model
# posterior <- run.jags(
#   model = JAGS_model_S_gamma_A_gamma,
#   data = data.list,
#   monitor = params,
#   inits = c(inits1, inits2, inits3, inits4),
#   n.chains = 4,
#   adapt = 1000,
#   burnin = 5000,
#   sample = 5000,
#   thin = 10,
#   method = "parallel"
# )
# 
# save(posterior, file = "E:\\Thesis R Data\\JAGS_model_S_gamma_A_gamma_var_0_point_1.RData")

load("E:\\Thesis R Data\\JAGS_model_S_gamma_A_gamma_var_0_point_1.RData")

# Put samples in dataframe
as.mcmc.list(posterior)[[1]] |> as.data.frame() -> AS

AS <-
  rbind(
    as.mcmc.list(posterior)[[2]] |> as.data.frame(),
    as.mcmc.list(posterior)[[3]] |> as.data.frame(),
    as.mcmc.list(posterior)[[4]] |> as.data.frame()
  )

# Extract matrices
A_point_1 <- AS[, (1:p)]
S_point_1 <- AS[, ((p+1):ncol(AS))]
```

```{r}
# # Var 5 Gamma simulation
# bases <- matrix(c(df1$Intensity,df2$Intensity), nrow = 2, ncol = length(df2$Intensity), byrow = T)
# mat <- as.matrix(mixed_spectrum) |> t()
# mat <- t(apply(mat, 1, scale_row_S)) + E
# nmf.out <- cnmf(mat, maxit = 10000, q = 2, s = bases)
# 
# x <- nmf.out$x
# ed <- nmf.out$ed
# w <- nmf.out$w
# a <- nmf.out$a
# s <- nmf.out$s
# reconstructedx <- nmf.out$reconstructedx
# s_init <- nmf.out$s_init
# 
# E <- 1 * 10 ^ -3
# 
# expectations_s <-
#   t(apply(s, 1, scale_row_S)) + E # added here for numerical reasons
# 
# var <- 5
# 
# alpha_s <- expectations_s ^ 2 / var
# beta_s <- expectations_s / var
# beta_s[beta_s == 0] <-  E
# alpha_s[alpha_s == 0] <-  E
# 
# var <- 1
# 
# expectations_a <- t(apply(w, 1, scale_row_A)) + E
# alpha_a <- expectations_a ^ 2 / var
# beta_a <- expectations_a / var
# beta_a[beta_a == 0] <-  E
# alpha_a[alpha_a == 0] <-  E
# 
# 
# p <- 2
# m <- 1
# n <- ncol(mat)
# 
# data.list = list(
#   X = mat,
#   m = m,
#   n = n,
#   p = p,
#   E = E,
#   alpha_s = alpha_s,
#   beta_s = beta_s,
#   alpha_a = alpha_a,
#   beta_a = beta_a
# )
# 
# 
# params <- c("A", "S")
# 
# inits1 <-
#   dump.format(list(
#     A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
#     S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
#     .RNG.name = "base::Wichmann-Hill",
#     .RNG.seed = 87460945
#   ))
# inits2 <-
#   dump.format(
#     list(
#       A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
#       S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
#       .RNG.name = "base::Marsaglia-Multicarry",
#       .RNG.seed = 874609
#     )
#   )
# inits3 <-
#   dump.format(list(
#     A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
#     S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
#     .RNG.name = "base::Super-Duper",
#     .RNG.seed = 8746
#   ))
# inits4 <-
#   dump.format(list(
#     A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
#     S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
#     .RNG.name = "base::Mersenne-Twister",
#     .RNG.seed = 874
#   ))
# 
# 
# # Run the model
# posterior <- run.jags(
#   model = JAGS_model_S_gamma_A_gamma,
#   data = data.list,
#   monitor = params,
#   inits = c(inits1, inits2, inits3, inits4),
#   n.chains = 4,
#   adapt = 1000,
#   burnin = 5000,
#   sample = 5000,
#   thin = 10,
#   method = "parallel"
# )
# 
# save(posterior, file = "E:\\Thesis R Data\\JAGS_model_S_gamma_A_gamma_var_5.RData")

load("E:\\Thesis R Data\\JAGS_model_S_gamma_A_gamma_var_5.RData")

# Put samples in dataframe
as.mcmc.list(posterior)[[1]] |> as.data.frame() -> AS

AS <-
  rbind(
    as.mcmc.list(posterior)[[2]] |> as.data.frame(),
    as.mcmc.list(posterior)[[3]] |> as.data.frame(),
    as.mcmc.list(posterior)[[4]] |> as.data.frame()
  )

# Extract matrices
A5 <- AS[, (1:p)]
S5 <- AS[, ((p+1):ncol(AS))]
```

```{r}
# Var 10 Gamma simulation
# bases <- matrix(c(df1$Intensity,df2$Intensity), nrow = 2, ncol = length(df2$Intensity), byrow = T)
# mat <- as.matrix(mixed_spectrum) |> t()
# mat <- t(apply(mat, 1, scale_row_S)) + E
# nmf.out <- cnmf(mat, maxit = 10000, q = 2, s = bases)
# 
# x <- nmf.out$x
# ed <- nmf.out$ed
# w <- nmf.out$w
# a <- nmf.out$a
# s <- nmf.out$s
# reconstructedx <- nmf.out$reconstructedx
# s_init <- nmf.out$s_init
# 
# E <- 1 * 10 ^ -3
# 
# expectations_s <-
#   t(apply(s, 1, scale_row_S)) + E # added here for numerical reasons
# 
# var <- 10
# 
# alpha_s <- expectations_s ^ 2 / var
# beta_s <- expectations_s / var
# beta_s[beta_s == 0] <-  E
# alpha_s[alpha_s == 0] <-  E
# 
# var <- 1
# 
# expectations_a <- t(apply(w, 1, scale_row_A)) + E
# alpha_a <- expectations_a ^ 2 / var
# beta_a <- expectations_a / var
# beta_a[beta_a == 0] <-  E
# alpha_a[alpha_a == 0] <-  E
# 
# 
# p <- 2
# m <- 1
# n <- ncol(mat)
# 
# data.list = list(
#   X = mat,
#   m = m,
#   n = n,
#   p = p,
#   E = E,
#   alpha_s = alpha_s,
#   beta_s = beta_s,
#   alpha_a = alpha_a,
#   beta_a = beta_a
# )
# 
# 
# params <- c("A", "S")
# 
# inits1 <-
#   dump.format(list(
#     A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
#     S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
#     .RNG.name = "base::Wichmann-Hill",
#     .RNG.seed = 87460945
#   ))
# inits2 <-
#   dump.format(
#     list(
#       A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
#       S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
#       .RNG.name = "base::Marsaglia-Multicarry",
#       .RNG.seed = 874609
#     )
#   )
# inits3 <-
#   dump.format(list(
#     A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
#     S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
#     .RNG.name = "base::Super-Duper",
#     .RNG.seed = 8746
#   ))
# inits4 <-
#   dump.format(list(
#     A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
#     S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
#     .RNG.name = "base::Mersenne-Twister",
#     .RNG.seed = 874
#   ))
# 
# 
# # Run the model
# posterior <- run.jags(
#   model = JAGS_model_S_gamma_A_gamma,
#   data = data.list,
#   monitor = params,
#   inits = c(inits1, inits2, inits3, inits4),
#   n.chains = 4,
#   adapt = 1000,
#   burnin = 5000,
#   sample = 5000,
#   thin = 10,
#   method = "parallel"
# )
# 
# save(posterior, file = "E:\\Thesis R Data\\JAGS_model_S_gamma_A_gamma_var_10.RData")

load("E:\\Thesis R Data\\JAGS_model_S_gamma_A_gamma_var_10.RData")

# Put samples in dataframe
as.mcmc.list(posterior)[[1]] |> as.data.frame() -> AS

AS <-
  rbind(
    as.mcmc.list(posterior)[[2]] |> as.data.frame(),
    as.mcmc.list(posterior)[[3]] |> as.data.frame(),
    as.mcmc.list(posterior)[[4]] |> as.data.frame()
  )

# Extract matrices
A10 <- AS[, (1:p)]
S10 <- AS[, ((p+1):ncol(AS))]
```

### Exponential

Seems like changing the variance does \_\_\_. What if we don't constrain the variance at all and do an expoential distribution

```{r}
# bases <- matrix(c(df1$Intensity,df2$Intensity), nrow = 2, ncol = length(df2$Intensity), byrow = T)
# mat <- as.matrix(mixed_spectrum) |> t()
# mat <- t(apply(mat, 1, scale_row_S)) + E
# nmf.out <- cnmf(mat, maxit = 10000, q = 2, s = bases)
# 
# x <- nmf.out$x
# ed <- nmf.out$ed
# w <- nmf.out$w
# a <- nmf.out$a
# s <- nmf.out$s
# reconstructedx <- nmf.out$reconstructedx
# s_init <- nmf.out$s_init
# 
# E <- 1 * 10 ^ -3
# 
# expectations_s <-
#   1/t(apply(s, 1, scale_row_S)) + E # added here for numerical reasons
# 
# 
# var <- 1
# 
# expectations_a <- t(apply(w, 1, scale_row_A)) + E
# alpha_a <- expectations_a ^ 2 / var
# beta_a <- expectations_a / var
# beta_a[beta_a == 0] <-  E
# alpha_a[alpha_a == 0] <-  E
# 
# expectations_s[which(is.infinite(expectations_s))] <- max(expectations_s[-which(is.infinite(expectations_s))])
# 
# p <- 2
# m <- 1
# n <- ncol(mat)
# 
# data.list = list(
#   X = mat,
#   m = m,
#   n = n,
#   p = p,
#   E = E,
#   lambda_s = expectations_s,
#   alpha_a = alpha_a,
#   beta_a = beta_a
# )
# 
# 
# params <- c("A", "S")
# 
# inits1 <-
#   dump.format(list(
#     A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
#     S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
#     .RNG.name = "base::Wichmann-Hill",
#     .RNG.seed = 87460945
#   ))
# inits2 <-
#   dump.format(
#     list(
#       A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
#       S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
#       .RNG.name = "base::Marsaglia-Multicarry",
#       .RNG.seed = 874609
#     )
#   )
# inits3 <-
#   dump.format(list(
#     A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
#     S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
#     .RNG.name = "base::Super-Duper",
#     .RNG.seed = 8746
#   ))
# inits4 <-
#   dump.format(list(
#     A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
#     S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
#     .RNG.name = "base::Mersenne-Twister",
#     .RNG.seed = 874
#   ))
# 
# 
# # Run the model
# posterior <- run.jags(
#   model = JAGS_model_S_exp_A_gamma,
#   data = data.list,
#   monitor = params,
#   inits = c(inits1, inits2, inits3, inits4),
#   n.chains = 4,
#   adapt = 1000,
#   burnin = 5000,
#   sample = 5000,
#   thin = 10,
#   method = "parallel"
# )
# 
# save(posterior, file = "E:\\Thesis R Data\\JAGS_model_S_exp_A_gamma_sim_gbrnmf_init.RData")

load("E:\\Thesis R Data\\JAGS_model_S_exp_A_gamma_sim_gbrnmf_init.RData")

# Put samples in dataframe
as.mcmc.list(posterior)[[1]] |> as.data.frame() -> AS

AS <-
  rbind(
    as.mcmc.list(posterior)[[2]] |> as.data.frame(),
    as.mcmc.list(posterior)[[3]] |> as.data.frame(),
    as.mcmc.list(posterior)[[4]] |> as.data.frame()
  )

# Extract matrices
A <- AS[, (1:p)]
S <- AS[, ((p+1):ncol(AS))]
```

## Sensitivity Analysis of A

There is little discussion of the sensitivity of the model to the prior choice of A. The most common choices are Gamma and Exponential distributions. Generally the Gamma distributed models have their shape such that each variable closely resembles an exponential rnadom variable with constrained variance. Of course opting for the exponential distribution removes the possibility of setting the variance of the matrix entry. While both these distirubtions encode the nonnegativity sought out in these models, they fail to act as a proper mixing proportion, often summing well beyond one. Therefore a natural modification to the model is to place a Dirichlet prior on the distribution of A thus enforcing summation to one. In the following convergence properties are assesed for each prior specification and Bayes factors are calculated to perform a sensitivity analysis.

### Gamma

TODO need to check Gamma at different variances

### Exponential

We begin by considering A values with an exponential distribution

... formulas and theory here...

```{r}
# bases <- matrix(c(df1$Intensity,df2$Intensity), nrow = 2, ncol = length(df2$Intensity), byrow = T)
# mat <- as.matrix(mixed_spectrum) |> t()
# mat <- t(apply(mat, 1, scale_row_S)) + E
# nmf.out <- cnmf(mat, maxit = 10000, q = 2, s = bases)
# 
# x <- nmf.out$x
# ed <- nmf.out$ed
# w <- nmf.out$w
# a <- nmf.out$a
# s <- nmf.out$s
# reconstructedx <- nmf.out$reconstructedx
# s_init <- nmf.out$s_init
# 
# E <- 1 * 10 ^ -3
# 
# expectations_s <-
#   t(apply(s, 1, scale_row_S)) + E # added here for numerical reasons
# 
# var <- 1
# 
# alpha_s <- expectations_s ^ 2 / var
# beta_s <- expectations_s / var
# beta_s[beta_s == 0] <-  E
# alpha_s[alpha_s == 0] <-  E
# 
# expectations_a <- (1/t(apply(w, 1, scale_row_A))) + E
# 
# 
# 
# p <- 2
# m <- 1
# n <- ncol(mat)
# 
# data.list = list(
#   X = mat,
#   m = m,
#   n = n,
#   p = p,
#   E = E,
#   alpha_s = alpha_s,
#   beta_s = beta_s,
#   lambda_a = expectations_a
# )
# 
# 
# params <- c("A", "S")
# 
# inits1 <-
#   dump.format(list(
#     A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
#     S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
#     .RNG.name = "base::Wichmann-Hill",
#     .RNG.seed = 87460945
#   ))
# inits2 <-
#   dump.format(
#     list(
#       A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
#       S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
#       .RNG.name = "base::Marsaglia-Multicarry",
#       .RNG.seed = 874609
#     )
#   )
# inits3 <-
#   dump.format(list(
#     A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
#     S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
#     .RNG.name = "base::Super-Duper",
#     .RNG.seed = 8746
#   ))
# inits4 <-
#   dump.format(list(
#     A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
#     S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
#     .RNG.name = "base::Mersenne-Twister",
#     .RNG.seed = 874
#   ))
# 
# 
# # Run the model
# posterior <- run.jags(
#   model = JAGS_model_S_gamma_A_exp,
#   data = data.list,
#   monitor = params,
#   inits = c(inits1, inits2, inits3, inits4),
#   n.chains = 4,
#   adapt = 1000,
#   burnin = 5000,
#   sample = 5000,
#   thin = 10,
#   method = "parallel"
# )
# 
# save(posterior, file = "E:\\Thesis R Data\\JAGS_model_S_gamma_A_exp_sim_gbrnmf_init.RData")

load("E:\\Thesis R Data\\JAGS_model_S_gamma_A_exp_sim_gbrnmf_init.RData")

# Put samples in dataframe
as.mcmc.list(posterior)[[1]] |> as.data.frame() -> AS

AS <-
  rbind(
    as.mcmc.list(posterior)[[2]] |> as.data.frame(),
    as.mcmc.list(posterior)[[3]] |> as.data.frame(),
    as.mcmc.list(posterior)[[4]] |> as.data.frame()
  )

# Extract matrices
A <- AS[, (1:p)]
S <- AS[, ((p+1):ncol(AS))]
```

```{r}
res <- get_plots_95_ci_spec(3,S,"median")
wavelength <-  1:length(mixed_spectrum)

g1 <- plot_95_ci_spec(res, wavelength, 1)
g2 <- plot_95_ci_spec(res, wavelength, 2)
# g3 <- plot_95_ci_spec(res, wavelength, 3)



grid.arrange(g1, g2, nrow = 1, ncol = 2)
```

```{r}
A_list <- get_A_values_and_median(A, n = 1,"median")
g1 <- get_plots_A_hist(A_list,i=1)
g2 <- get_plots_A_hist(A_list,i=2)

grid.arrange(g1, g2, nrow = 1, ncol = 2)
```

TO DO support that selecting constraining variance didn't matter vary the gamma variance as well.

```{r}
plot(posterior,vars=c("S[1,1]"))
```

```{r}
plot(posterior,vars=c("A[1,1]"))
```

### Dirichlet

We begin by considering A values with a Dirichlet distribution 6+ ... formulas and theory here...

```{r}
# bases <- matrix(c(df1$Intensity,df2$Intensity), nrow = 2, ncol = length(df2$Intensity), byrow = T)
# mat <- as.matrix(mixed_spectrum) |> t()
# mat <- t(apply(mat, 1, scale_row_S)) + E
# nmf.out <- cnmf(mat, maxit = 10000, q = 2, s = bases)
# 
# x <- nmf.out$x
# ed <- nmf.out$ed
# w <- nmf.out$w
# a <- nmf.out$a
# s <- nmf.out$s
# reconstructedx <- nmf.out$reconstructedx
# s_init <- nmf.out$s_init
# 
# E <- 1 * 10 ^ -3
# 
# expectations_s <-
#   t(apply(s, 1, scale_row_S)) + E # added here for numerical reasons
# 
# var <- 1
# 
# alpha_s <- expectations_s ^ 2 / var
# beta_s <- expectations_s / var
# beta_s[beta_s == 0] <-  E
# alpha_s[alpha_s == 0] <-  E
# 
# expectations_a <- w/sum(w)
# 
# 
# 
# p <- 2
# m <- 1
# n <- ncol(mat)
# 
# data.list = list(
#   X = mat,
#   m = m,
#   n = n,
#   p = p,
#   E = E,
#   alpha_s = alpha_s,
#   beta_s = beta_s,
#   alpha_a = expectations_a
# )
# 
# 
# params <- c("A", "S")
# 
# inits1 <-
#   dump.format(list(
#     A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
#     S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
#     .RNG.name = "base::Wichmann-Hill",
#     .RNG.seed = 87460945
#   ))
# inits2 <-
#   dump.format(
#     list(
#       A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
#       S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
#       .RNG.name = "base::Marsaglia-Multicarry",
#       .RNG.seed = 874609
#     )
#   )
# inits3 <-
#   dump.format(list(
#     A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
#     S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
#     .RNG.name = "base::Super-Duper",
#     .RNG.seed = 8746
#   ))
# inits4 <-
#   dump.format(list(
#     A = matrix(runif(m * p, 0.1, 1), nrow = m, ncol = p),
#     S = matrix(runif(p * n, 0.1, 1), nrow = p, ncol = n),
#     .RNG.name = "base::Mersenne-Twister",
#     .RNG.seed = 874
#   ))
# 
# 
# # Run the model
# posterior <- run.jags(
#   model = JAGS_model_S_gamma_A_dir,
#   data = data.list,
#   monitor = params,
#   inits = c(inits1, inits2, inits3, inits4),
#   n.chains = 4,
#   adapt = 10000,
#   burnin = 10000,
#   sample = 10000,
#   thin = 10,
#   method = "parallel"
# )
# 
# save(posterior, file = "E:\\Thesis R Data\\JAGS_model_S_gamma_A_dir_sim_gbrnmf_init.RData")

load("E:\\Thesis R Data\\JAGS_model_S_gamma_A_dir_sim_gbrnmf_init.RData")

# Put samples in dataframe
as.mcmc.list(posterior)[[1]] |> as.data.frame() -> AS

AS <-
  rbind(
    as.mcmc.list(posterior)[[2]] |> as.data.frame(),
    as.mcmc.list(posterior)[[3]] |> as.data.frame(),
    as.mcmc.list(posterior)[[4]] |> as.data.frame()
  )

# Extract matrices
A <- AS[, (1:p)]
S <- AS[, ((p+1):ncol(AS))]
```

```{r}
res <- get_plots_95_ci_spec(3,S,"median")
wavelength <-  1:length(mixed_spectrum)

g1 <- plot_95_ci_spec(res, wavelength, 1)
g2 <- plot_95_ci_spec(res, wavelength, 2)
# g3 <- plot_95_ci_spec(res, wavelength, 3)



grid.arrange(g1, g2, nrow = 1, ncol = 2)
```

```{r}
A_list <- get_A_values_and_median(A, n = 1,"median")
g1 <- get_plots_A_hist(A_list,i=1)
g2 <- get_plots_A_hist(A_list,i=2)

grid.arrange(g1, g2, nrow = 1, ncol = 2)
```

```{r}
plot(posterior,vars=c("S[1,1]"))
```

```{r}
plot(posterior,vars=c("A[1,1]"))
```

Dirichlet messes up the model big time.

TODO run this again to double check

## Robust Likelihoods

It is well known Normal likelihoods often fail to be robust in the presecense of outliers \[Gill's book citation\]. A strategy suggested in \[Gills textbook\] is to play a student-T distribution on the likelihood and compare models with varying degrees of freedom. This allows for the investiation of ... need to read more about this.

Error tails will be fatter which means they may be more robust and the non error terms could get estimated better.

Could also try skewed error.

# Application

To display the improved practicality of a Gamma-Gamma BPSS approach to GBR-NMF, the analysis of Citric acid, Glucose, Glycine, and Serine as conducted in [@milligan2023reconstruction], is replicated. First spectra are processed through GBR-NMF. This results in a matrix of latent factors used to provide the prior information for the spectra matrix and a matrix of scores used to initialize the mixing matrix. Specifically, the expectation of each Gamma random variable is set to its corresponding initialization value. Additionally, the variance is set equal to 1.

TODO just because we set variance to 1 doesn't mean it stays a one after convergence right?

## Data

TODO how much info is needed in introducing the data. I've omitted the specifications of the Raman spectroscopy collection and opted to just explain it.

Data for the Raman spectra of Citric acid, Glucose, Glycine, and Serine was provided from [@milligan2023reconstruction]. For each biochemical, spectra of the solid form and liquid form, dissolved in deionized water, were collected. 5 mixtures were then created in varying concentrations of each biochemical,

```{r}
# Load necessary libraries
library(ggplot2)
library(tidyr)

# Correct the data organization based on your description
values <- c(A1, A2, A3, A4, A5)
conditions <- rep(c("A1", "A2", "A3", "A4", "A5"), each = 4)
chemicals <- rep(c("Citric acid", "Glucose", "Glycine", "Serine"), times = 5)

# Create a data frame
data_df <- data.frame(Condition = conditions, Chemical = chemicals, Value = values)

# Plotting
ggplot(data_df, aes(x = Condition, y = Value, fill = Chemical)) +
  geom_bar(stat = "identity",
           position = position_dodge(),
           alpha = 0.8) +
  scale_fill_manual(
    values = c(
      "Citric acid" = "#D1BCE3",
      "Glucose" = "#C49BBB",
      "Glycine" = "#A1867F",
      "Serine" = "#585481"
    )
  ) +
  labs(title = "Biochemical Mixture Concentrations",
       x = "Mixtures",
       y = "% Total Molarity",
       fill = "Chemical") +
  theme_minimal()



```


```{r}
pure_df <-  read.csv("..\\data\\4BasesSolid_CA-Glu-Gly-Ser.csv",header = FALSE)
titles <- c("Citric acid", "Glucose", "Glycine", "Serine")

library(ggplot2)
library(gridExtra)

# Assuming pure_df has 4 rows, with each row being a spectrum and columns being the measurement points

# Create a list to store plots
plot_list <- vector("list", 4)

# Loop through each row to create a plot for each spectrum
for (i in 1:4) {
  spectrum_data <- t(pure_df[i,]) |> as.data.frame()
  colnames(spectrum_data) <- c("Intensity") 
  spectrum_data$RamanShift <- 1:ncol(pure_df)
  
  plot_list[[i]] <- ggplot(spectrum_data, aes(x = RamanShift, y = Intensity)) +
    geom_line() +
    labs(title = titles[i]) +
    theme_minimal()
}

# Arrange the plots in a 2x2 grid
grid_arrange_shared_legend <- do.call(gridExtra::grid.arrange, c(plot_list, ncol = 2))

```

Dissolving these biochemicals in deionized water yields the following spectra

```{r}
pure_diss_df <-  read.csv("..\\data\\4BasesLiquid_CA-Glu-Gly-Ser.csv",header = FALSE)

plot_list <- vector("list", 4)

# Loop through each row to create a plot for each spectrum
for (i in 1:4) {
  spectrum_data <- t(pure_diss_df[i,]) |> as.data.frame()
  colnames(spectrum_data) <- c("Intensity") 
  spectrum_data$RamanShift <- 1:ncol(pure_diss_df)
  
  plot_list[[i]] <- ggplot(spectrum_data, aes(x = RamanShift, y = Intensity)) +
    geom_line() +
    labs(title = titles[i]) +
    theme_minimal()
}

# Arrange the plots in a 2x2 grid
grid_arrange_shared_legend <- do.call(gridExtra::grid.arrange, c(plot_list, ncol = 2))
```

The following data was the data put into GBR-NMF:

```{r}
misc_gbrnmf <-  read.csv(".\\data\\Input Data for NMF\\Input Data for NMF\\2. Misc q=4 and q=4+ solution bases\\nmf_data_CA_Glu_Gly_Ser.csv",header = FALSE)
head(misc_gbrnmf[,1:6])
```

There are 3 replicates of 5 mixtures of the same 4 solutions at varying concentrations. This dataframe corresponds to A1-A5 as in the paper. Replicate means they remade the solution and took a Raman spectrum.

```{r}
par(mfrow=c(2,2))
titles <- sapply(1:5, function(i) paste0("A", i, "-", 1:3)) |> as.vector()
for (j in c(1,4,7,10,13)) {
  plot(1:length(misc_gbrnmf[j,]),as.numeric(misc_gbrnmf[j,]),type="l",ylab="Intensity",xlab="Index",col=1,lty=1) 
for (i in (j+1):(j+2)) {
  lines(1:length(misc_gbrnmf[i,]),as.numeric(misc_gbrnmf[i,]),col=i,lty=i)
}
legend("topright", legend=titles[j:(j+2)], col=1:3, lty=1:3)
}
```

This structure is followed for the B's (Lipids) and the C's (TCA). They were looking to see if they could recover the concentrations of the solutions from the mixture by telling NMF which bases signals were present. We want to see if there are Bayesian techniques to deconvolve this mixture into individual signals and see if we can tell the concentration.

We are going to do a PBSS on the Raman spectra using JAGS.

TODO

Talk about the implications of a narrow CI

Talk about the implications of a wide CI

run 3 given spectra and see if it can 4

run 4 given spectra looking for 5 and see what happens

# Conclusion

what you did and so what

This work presents the efficacy of using a supervised NMF approach to initialize a BPSS model.

Future work to be done.

TODO check if this is right: Gamma random variables in model are independent, would be useful to allow for correlation since NMF latent factors are correlated. Skewed likelihood for the error could be investigated.

# References

::: {#refs}
:::

# Appendix

## Jags code

```{r}
# Gamma Model code
JAGS_model_S_gamma_A_gamma <- "
model {
    # Likelihood
    for (i in 1:m) {
        for (k in 1:n) {
            X[i, k] ~ dnorm(mu[i, k], tau[i])
            mu[i, k] <- inprod(A[i, ], S[, k])
        }
    }

    # Prior distributions for pure spectra (S) and mixing coefficients (A)
    for (j in 1:p) {
        for (k in 1:n) {
            S[j, k] ~ dgamma(alpha_s[j, k], beta_s[j, k]) T(0.001, 1.001)
        }
    }
    
    for (i in 1:m) {
      for (j in 1:p) {
        A[i, j] ~ dgamma(alpha_a[i,j], beta_a[i,j]) T(0.001,1.001)
      }
    }
    

    # Hyperparameters for the Gamma distributions for each element of S
    for (j in 1:p) {
        for (k in 1:n) {
            alpha_s[j, k] ~ dgamma(2, E)
            beta_s[j, k] ~ dgamma(2, E)
        }
    }
      for (i in 1:m) {
          for (j in 1:p) {
            alpha_a[i,j] ~ dgamma(2, E)
            beta_a[i,j] ~ dgamma(2, E)
          }
        }
    

    # Noise variances
    for (i in 1:m) {
        tau[i] ~ dgamma(2, E)
    }
}
"

JAGS_model_S_exp_A_gamma <- "
model {
    # Likelihood
    for (i in 1:m) {
        for (k in 1:n) {
            X[i, k] ~ dnorm(mu[i, k], tau[i])
            mu[i, k] <- inprod(A[i, ], S[, k])
        }
    }
    

    # Prior distributions for pure spectra (S) and mixing coefficients (A)
    for (j in 1:p) {
        for (k in 1:n) {
            S[j, k] ~ dexp(lambda_s[j,k]) T(0.001, 1.001)
        }
    }
    
    for (i in 1:m) {
      for (j in 1:p) {
        A[i, j] ~ dgamma(alpha_a[i,j], beta_a[i,j]) T(0.001,1.001)
      }
    }
    
  
    # Hyperparameters for the Gamma distributions for each element of S
    for (j in 1:p) {
        for (k in 1:n) {
           lambda_s[j,k] ~ dgamma(2, E) # Assuming a gamma prior for the rate parameters of the Exponential distribution
        }
    }
    
      for (i in 1:m) {
          for (j in 1:p) {
            alpha_a[i,j] ~ dgamma(2, E)
            beta_a[i,j] ~ dgamma(2, E)
          }
        }
    

    # Noise variances
    for (i in 1:m) {
        tau[i] ~ dgamma(2, E)
    }
}
"

JAGS_model_S_gamma_A_dir <- "
model {
    # Likelihood
    for (i in 1:m) {
        for (k in 1:n) {
            X[i, k] ~ dnorm(mu[i, k], tau[i])
            mu[i, k] <- inprod(A[i, ], S[, k])
        }
    }

    # Prior distributions for pure spectra (S)
    for (j in 1:p) {
        for (k in 1:n) {
            S[j, k] ~ dgamma(alpha_s[j, k], beta_s[j, k]) T(0.001, 1.001)
        }
    }
    
    # Corrected Dirichlet prior for mixing coefficients (A)
    for (i in 1:m) {
        A[i, 1:p] ~ ddirich(alpha_a[i, 1:p]) # Ensure alpha_a[i, 1:p] is a vector of concentration parameters for the i-th row of A
    }
    
    # Hyperparameters for the Gamma distributions for each element of S and Dirichlet parameters for A
    for (j in 1:p) {
        for (k in 1:n) {
            alpha_s[j, k] ~ dgamma(2, E)
            beta_s[j, k] ~ dgamma(2, E)
        }
    }
    
     for (i in 1:m) {
      for (j in 1:p) {
        alpha_a[i, j] ~ dgamma(2, E)
      }
    }

    # Noise variances
    for (i in 1:m) {
        tau[i] ~ dgamma(2, E)
    }
}
"


JAGS_model_S_gamma_A_exp <- "
model {
    # Likelihood
    for (i in 1:m) {
        for (k in 1:n) {
            X[i, k] ~ dnorm(mu[i, k], tau[i])
            mu[i, k] <- inprod(A[i, ], S[, k])
        }
    }

    # Prior distributions for pure spectra (S)
    for (j in 1:p) {
        for (k in 1:n) {
            S[j, k] ~ dgamma(alpha_s[j, k], beta_s[j, k]) T(0.001, 1.001)
        }
    }
    
    # Prior distributions for mixing coefficients (A) using Exponential distribution
    for (i in 1:m) {
      for (j in 1:p) {
        A[i, j] ~ dexp(lambda_a[i,j]) T(0.001, 1.001)
      }
    }
    
    # Hyperparameters for the Gamma distributions for each element of S
    # and for the Exponential distributions for each element of A
    for (j in 1:p) {
        for (k in 1:n) {
            alpha_s[j, k] ~ dgamma(2, E)
            beta_s[j, k] ~ dgamma(2, E)
        }
    }
    for (i in 1:m) {
        for (j in 1:p) {
            lambda_a[i,j] ~ dgamma(2, E) # Assuming a gamma prior for the rate parameters of the Exponential distribution
        }
    }

    # Noise variances
    for (i in 1:m) {
        tau[i] ~ dgamma(2, E)
    }
}
"




```

## Convergence Diagnostics
